{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badf8af3",
   "metadata": {},
   "source": [
    "## LLM API call using OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940ab0b",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1bf645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import base64\n",
    "\n",
    "# for plotting images\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbeae9",
   "metadata": {},
   "source": [
    "### OpenAI client setup\n",
    "\n",
    "In order to use OpenAI API you will need an OPENAI_API_KEY. You then need to create .env file with this line inside:\n",
    "\n",
    "``OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\" ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the env variable (API key)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=os.path.join(os.getcwd(), \"..\", \".env\"), override=True) # works if the .env file is stored in the main project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cabcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use environment variables\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# construct client instance\n",
    "client = OpenAI(api_key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b1ba6",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "`*sample_path*` is where a sample used for prediction is stored. For now, a random image from the internet (of a likely melanoma) is used to test the system.\n",
    "\n",
    "`*sample_probs_path*` is where probabilities output by the model for the sample are stored. \n",
    "\n",
    "`*xia_gradcam_output_path*` and `*xia_shap_output_path*` refer to the locations where xai output images are stored. Currently, I am using test images generated with ChatGPT.\n",
    "\n",
    "`*xai_influence_output_path*` is where a mock output from the influence function is stored. Actual output should rank scores for training cases (samples) from the most to the least influencial for the current prediction. For our survey we can present 10 top influencial, but for the LLM call we should pass top 100 influencial cases (ranked by absolute scores), the model can then run some statistical analysis.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113fbb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = os.path.join(os.getcwd(), '../user_inputs/user_sample1.jpg')\n",
    "sample_probs_path = os.path.join(os.getcwd(), '../results/xai_output/model_output.csv')  # Probabilities output by the model for the sample\n",
    "\n",
    "xai_gradcam_output_path = os.path.join(os.getcwd(), '../results/xai_output/user_sample1_xai_gradcam.png')\n",
    "xai_shap_output_path = os.path.join(os.getcwd(), '../results/xai_output/user_sample1_xai_shap.png')\n",
    "xai_influence_output_path = os.path.join(os.getcwd(), '../results/xai_output/user_sample1_influence_function.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the images to base64 byte objects in string format\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# encode the xai output images\n",
    "xai_gradcam_enc = encode_image(xai_gradcam_output_path)\n",
    "xai_shap_enc = encode_image(xai_shap_output_path)\n",
    "\n",
    "# Read in the influence function output\n",
    "with open(xai_influence_output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    xai_influence_function = f.read()\n",
    "\n",
    "# Read in the sample probabilities output by the model\n",
    "with open(sample_probs_path, \"r\",  encoding=\"utf-8\") as f:\n",
    "    sample_probs = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7513ae0",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "In constructing the instructions for LLM, I've followed OpenAI's best prompting practices available [here](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the instructions\n",
    "with open(os.path.join(os.getcwd(), \"../data/llm/llm_instructions.md\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    instructions = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d205bd",
   "metadata": {},
   "source": [
    "### LLM API call\n",
    "\n",
    "We use a flagship model from OpenAI, GPT-4.1 (model snapshot: 2025-04-14). According to the company's documentation, it is highly capable at complex task while expressing strict adherance to instructions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-2025-04-14\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\" : instructions\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": str(sample_probs) }, # sample probabilities\n",
    "                { \n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": xai_influence_function }, # influence function output\n",
    "                    \n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/png;base64,{xai_gradcam_enc}\", # GradCAM output, base64-encoded PNG file\n",
    "                    \"detail\": \"auto\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/png;base64,{xai_shap_enc}\", # SHAP output, base64-encoded PNG file\n",
    "                    \"detail\": \"auto\"\n",
    "                },\n",
    "\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    top_p=0.005\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a569bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.usage.output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display xai images for context\n",
    "img1 = Image.open(sample_path)\n",
    "img2 = Image.open(xai_gradcam_output_path)\n",
    "img3 = Image.open(xai_shap_output_path)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(img1)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Sample')\n",
    "axes[1].imshow(img2)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('GradCAM Visualisation')\n",
    "axes[2].imshow(img3)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('SHAP Visualisation')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
