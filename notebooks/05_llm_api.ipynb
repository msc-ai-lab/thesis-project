{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "badf8af3",
   "metadata": {},
   "source": [
    "## LLM API call using OpenAI API\n",
    "\n",
    "The purpose of this notebook is to demonstrate the use of LLM API calls for increasing understandibility of the XAI methods in the context of CNN skin cancer prediction. In each individual API call, the LLM model is supplied with a set of thorough instructions aimed at making its outputs more accessible to non-technical audiences. \n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "Please note, this LLM pipeline uses data output by the ``04_xai_pipeline.ipynb`` notebook, therefore, please make sure the files are available in relevant folders.\n",
    "For the analysis, you also need to supply your own sample that needs to be stored in ``user_inputs`` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940ab0b",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1bf645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional, Annotated\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b1ba6",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113fbb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the relevant folders\n",
    "root_dir = Path.cwd().parent\n",
    "user_inputs_dir = root_dir / 'user_inputs'\n",
    "results_dir = root_dir / 'results'\n",
    "xai_output_dir = results_dir / 'xai_output'\n",
    "\n",
    "# Define relevant input data paths\n",
    "sample_path = user_inputs_dir / 'user_sample1.jpg'\n",
    "sample_probs_path = xai_output_dir / 'model_output.csv'\n",
    "xai_gradcam_output_path = xai_output_dir / 'user_sample1_xai_gradcam.png'\n",
    "xai_shap_output_path = xai_output_dir / 'user_sample1_xai_shap.png'\n",
    "xai_influence_output_path = xai_output_dir / 'user_sample1_influence_function.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfbeae9",
   "metadata": {},
   "source": [
    "### OpenAI client setup\n",
    "\n",
    "In order to use OpenAI API you will need an OPENAI_API_KEY. You then need to create .env file with your own key, e.g.:\n",
    "\n",
    "``OPENAI_API_KEY=\"YOUR_OPENAI_API_KEY\" ``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0508791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define .env file path\n",
    "env_path = root_dir / '.env'\n",
    "\n",
    "# Load the .env variable (API key)\n",
    "load_dotenv(dotenv_path=env_path, \n",
    "            override=True) # Outputs True if variables could be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cabcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the loaded OpenAI API key\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Construct client instance using the key\n",
    "client = OpenAI(api_key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a7d5b",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "In this step we load the outputs from the 04_xai_pipeline.ipynb notebook and encode the images to the format supported by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the images to base64 byte objects in string format\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# encode the xai output images\n",
    "xai_gradcam_enc = encode_image(xai_gradcam_output_path)\n",
    "xai_shap_enc = encode_image(xai_shap_output_path)\n",
    "\n",
    "# encode the user sample\n",
    "user_sample_enc = encode_image(sample_path)\n",
    "\n",
    "# Read in the influence function output\n",
    "with open(xai_influence_output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    xai_influence_function = f.read()\n",
    "\n",
    "# Read in the sample probabilities output by the model\n",
    "with open(sample_probs_path, \"r\",  encoding=\"utf-8\") as f:\n",
    "    sample_probs = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7513ae0",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "In constructing the instructions for LLM, we have followed OpenAI's best prompting practices available [here](https://cookbook.openai.com/examples/gpt4-1_prompting_guide).\n",
    "\n",
    "Among the others, a few-shot learning technique was used in order for the model to learn patterns present in expected outputs.\n",
    "\n",
    "The instructions are stored in ``data/llm`` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the instructions\n",
    "with open(os.path.join(os.getcwd(), \"../data/llm/llm_instructions.md\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    instructions = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d205bd",
   "metadata": {},
   "source": [
    "### LLM API call\n",
    "\n",
    "We use a flagship model from OpenAI, GPT-4.1 (model snapshot: 2025-04-14). According to the company's documentation, it is highly capable at complex task while expressing strict adherance to instructions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac7d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4.1-2025-04-14\",\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"developer\",\n",
    "                \"content\" : instructions\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    { \n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": str(sample_probs) }, # sample probabilities\n",
    "                    { \n",
    "                        \"type\": \"input_text\",\n",
    "                        \"text\": xai_influence_function }, # influence function output\n",
    "                        \n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/png;base64,{xai_gradcam_enc}\", # GradCAM output, base64-encoded PNG file\n",
    "                        \"detail\": \"auto\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/png;base64,{xai_shap_enc}\", # SHAP output, base64-encoded PNG file\n",
    "                        \"detail\": \"auto\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"input_image\",\n",
    "                        \"image_url\": f\"data:image/jpg;base64,{user_sample_enc}\", # Original user sample, base64-encoded JPG file\n",
    "                        \"detail\": \"auto\"\n",
    "                    },\n",
    "\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    # Print the LLM interpretation\n",
    "    print(response.output_text)\n",
    "\n",
    "# Handle potential errors\n",
    "except TimeoutError as e:\n",
    "    print(f\"The LLM API call encountered TimeoutError: {e}\")\n",
    "\n",
    "except openai.APIError as e:\n",
    "    print(f\"OpenAI API error: {e}\")    \n",
    "\n",
    "except openai.APIConnectionError as e:\n",
    "    print(f\"Connection error: {e}\")\n",
    "    \n",
    "except openai.RateLimitError as e:\n",
    "    print(f\"Rate limit exceeded: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display xai images for context\n",
    "img1 = Image.open(sample_path)\n",
    "img2 = Image.open(xai_gradcam_output_path)\n",
    "img3 = Image.open(xai_shap_output_path)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(img1)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original Sample')\n",
    "axes[1].imshow(img2)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('GradCAM Visualisation')\n",
    "axes[2].imshow(img3)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('SHAP Visualisation')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41881756",
   "metadata": {},
   "source": [
    "### Write the LLM interpretaton to .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab82102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path for the LLM output\n",
    "llm_output_path = xai_output_dir / 'llm_output.txt'\n",
    "\n",
    "with open(file=llm_output_path, mode=\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4427cd0",
   "metadata": {},
   "source": [
    "### Parse LLM Output for Quantitative Analysis\n",
    "\n",
    "The call below parses the LLM interpretation while extracting cruicial insights to enable a quantitative analysis against the original CNN prediction. \n",
    "\n",
    "Please note, according to llm_instructons.md we expect the LLM to interpret predictions output by the CNN that are `>= 0.5 and < 0.6` for the predicted class as **borderline**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb1c534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction(BaseModel):\n",
    "    prediction: Literal['Benign', 'Malignant'] = Field(\n",
    "        description=\"'Benign' for when the AI analysis suggests low or moderately low concern for malignancy; 'Malignant' for when the AI analysis indicates high or moderately high concern for malignancy.\"\n",
    "        )\n",
    "    borderline: Literal[True, False] = Field(\n",
    "        description=\"True if the model considers this to be a borderline case, False otherwise. Must be exactly True or False.\"\n",
    "    )\n",
    "    confidence: Annotated[float, Field(\n",
    "        description=\"Model confidence, as indicated in Confidence Level section, to 2 decimal places.\"\n",
    "        )]\n",
    "    influential_cases_percentage = Annotated[float, Field(\n",
    "        description=\"Influence Functions: What percentage of the most influential training cases share the same ground truth label as the predicted class. Output float with 2 decimal places.\"\n",
    "    )]\n",
    "\n",
    "try:\n",
    "    extraction = client.responses.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        input=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an expert at structured data extraction. You will be given unstructured text from AI analysis and you should convert it into the given structure.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": response.output_text\n",
    "                },\n",
    "        ],\n",
    "        text_format=Prediction,\n",
    "    )\n",
    "\n",
    "    extracted_data = extraction.output_parsed.model_dump()\n",
    "\n",
    "# Handle potential errors\n",
    "except TimeoutError as e:\n",
    "    print(f\"The LLM API call encountered TimeoutError: {e}\")\n",
    "\n",
    "except openai.APIError as e:\n",
    "    print(f\"OpenAI API error: {e}\")    \n",
    "\n",
    "except openai.APIConnectionError as e:\n",
    "    print(f\"Connection error: {e}\")\n",
    "    \n",
    "except openai.RateLimitError as e:\n",
    "    print(f\"Rate limit exceeded: {e}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca0601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path for the extracted data \n",
    "parsed_llm_output_path = xai_output_dir / 'parsed_llm_output.csv'\n",
    "\n",
    "# Write extracted_data to a csv file for quantitative analysis\n",
    "with open(file=parsed_llm_output_path, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(extracted_data.keys())\n",
    "    writer.writerow(extracted_data.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
