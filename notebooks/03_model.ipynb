{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eb5cd6",
   "metadata": {},
   "source": [
    "# Skin Cancer Detection using Deep Learning\n",
    "This Jupyter notebook demonstrates how to build a deep learning model for skin cancer detection using the Xception architecture. The model classifies skin lesion images as either benign or malignant.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Experiments\n",
    "This project utilises Weights & Biases (wandb) to experiment various model configurations. To perform experiments with different configurations, you need to follow the following steps first:\n",
    "\n",
    "- Create a wandb account at [wandb.ai](https://wandb.ai) if you don't have one yet\n",
    "- Get your Wandb API key from your account settings\n",
    "- Add your Wandb API key in the `.env` file with the key `WANDB_API_KEY`\n",
    "- If the `.env` file doesn't exist, copy the `.env.example` file to create `.env` and replace the placeholder with your actual API key\n",
    "\n",
    "### Environment Setup\n",
    "Install the required packages from `requirements.txt` file.\n",
    "\n",
    "### Notebook Structure\n",
    "This notebook is organized into the following sections:\n",
    "- Import Libraries - Required Python packages for the project\n",
    "- Constants - Define paths and other constants\n",
    "- Load Metadata - Load and explore the updated metadata\n",
    "- Split Training and Testing Set - Prepare data splits\n",
    "- Prepare Dataset - Custom dataset class for loading images\n",
    "- Data Augmentation - Apply transformations to improve model generalization\n",
    "- Create Data Loaders - Batch data loading for training\n",
    "- Train the Model - Functions for model training and validation\n",
    "- Save/Load the Model - Utilities to persist trained models\n",
    "- Test the Model - Evaluate model performance\n",
    "- Experiments - Run training with specific configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a25985",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9c11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dango\\OneDrive - UWE Bristol\\projects\\thesis-project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from wandb.sdk.wandb_run import Run\n",
    "\n",
    "# Local imports\n",
    "from scd.utils.common import get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe63bb",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a73123",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2 # Malignant and Benign\n",
    "random_state = 42\n",
    "model_type = 'ViT'\n",
    "\n",
    "# Define paths\n",
    "root_path = Path.cwd().parent\n",
    "model_dir = root_path / 'models' / 'temp'\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68e450",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4268802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = root_path / 'data' / 'processed'\n",
    "\n",
    "# Load the saved datasets\n",
    "train_data = torch.load(save_dir / model_type / 'train_dataset.pt')\n",
    "val_data = torch.load(save_dir / model_type / 'val_dataset.pt')\n",
    "test_data = torch.load(save_dir / model_type / 'test_dataset.pt')\n",
    "\n",
    "# Extract images and labels\n",
    "train_images, train_labels = train_data['images'], train_data['labels']\n",
    "val_images, val_labels = val_data['images'], val_data['labels']\n",
    "test_images, test_labels = test_data['images'], test_data['labels']\n",
    "\n",
    "# Create TensorDataset objects\n",
    "train_tensor_dataset = TensorDataset(train_images, train_labels)\n",
    "val_tensor_dataset = TensorDataset(val_images, val_labels)\n",
    "test_tensor_dataset = TensorDataset(test_images, test_labels)\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_tensor_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_tensor_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb2e4f",
   "metadata": {},
   "source": [
    "## Get Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244726d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "WANDB_API_KEY = os.getenv('WANDB_API_KEY')\n",
    "WANDB_ENTITY = os.getenv('WANDB_ENTITY')\n",
    "WANDB_PROJECT = os.getenv('WANDB_PROJECT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d760451",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68fdaeb",
   "metadata": {},
   "source": [
    "### Train Epoch\n",
    "\n",
    "The `train_epoch` function handles one complete training cycle through all batches in the training dataset. It tracks key metrics and reports them to Weights & Biases if an experiment run is provided for logging, enabling experiment tracking and visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e640903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, train_loader: DataLoader, criterion: nn.Module, optimiser: torch.optim.Optimizer, epochs: int, current_epoch: int, wandb_run: Run = None, phase: str = 'Training') -> tuple:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The model to train.\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for the training dataset.\n",
    "    criterion : nn.Module\n",
    "        Loss function to compute the loss.\n",
    "    optimiser : torch.optim.Optimizer\n",
    "        Optimiser for updating model weights.\n",
    "    epochs : int\n",
    "        Total number of epochs for training.\n",
    "    current_epoch : int\n",
    "        Current epoch number.\n",
    "    wandb_run : Run, optional\n",
    "        Weights & Biases run object for logging metrics. Defaults to None.\n",
    "    phase : str, optional\n",
    "        Phase of training, e.g., 'Training' or 'Fine-tuning'. Defaults to 'Training\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing the mean loss and accuracy for the epoch.\n",
    "    \"\"\"\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize variables to track loss and accuracy\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Create progress bar for this epoch\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch [{current_epoch+1}/{epochs}]\", ncols=120)\n",
    "    \n",
    "    # Training loop\n",
    "    for i, (images, labels) in enumerate(train_pbar):\n",
    "        # Move data to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        # Back propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimiser.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss += loss.item()\n",
    "        mean_loss = running_loss / (i + 1)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        # Log metrics to Weights & Biases\n",
    "        if wandb_run:\n",
    "            wandb_run.log({\n",
    "                'train_loss': mean_loss,\n",
    "                'train_accuracy': accuracy,\n",
    "                'epoch': current_epoch + 1,\n",
    "                'phase': phase\n",
    "            })\n",
    "\n",
    "        # Update progress bar with training metrics    \n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f'{mean_loss:.4f}', \n",
    "            'acc': f'{accuracy:.2f}%',\n",
    "        })\n",
    "    \n",
    "    # Return the mean loss and accuracy for the epoch\n",
    "    return mean_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12445bd",
   "metadata": {},
   "source": [
    "### Validate Epoch\n",
    "\n",
    "The `validate_epoch` function evaluates the model's performance on validation data during training. It measures metrics like loss, accuracy, and ROC AUC score to track how well the model generalizes to unseen data. This function is crucial for monitoring model performance, detecting overfitting, and determining when to stop training or adjust hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac60905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model: nn.Module, val_loader: DataLoader, criterion: nn.Module, epoch: int, wandb_run: Run = None, phase:str = 'Validation') -> tuple:\n",
    "    \"\"\"\n",
    "    Validate the model for one epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The model to validate.\n",
    "    val_loader : DataLoader\n",
    "        DataLoader for the validation dataset.\n",
    "    criterion : nn.Module\n",
    "        Loss function to compute the loss.\n",
    "    epoch : int\n",
    "        Current epoch number.\n",
    "    wandb_run : Run, optional\n",
    "        Weights & Biases run object for logging metrics. Defaults to None.\n",
    "    phase : str, optional\n",
    "        Phase of validation, e.g., 'Validation' or 'Testing'. Defaults to 'Validation'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing the average validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to track loss and accuracy\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Create progress bar for validation\n",
    "    val_pbar = tqdm(val_loader, desc=phase, ncols=120)\n",
    "\n",
    "    # Disable gradient calculation for validation\n",
    "    with torch.no_grad():\n",
    "        # Validation loop\n",
    "        for images, labels in val_pbar:\n",
    "            # Move data to device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "\n",
    "            # Collect labels and predictions for ROC_AUC Score\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "            # Collect labels and predictions for ROC_AUC Score\n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(predicted)\n",
    "\n",
    "            # Update progress bar with validation metrics\n",
    "            val_pbar.set_postfix({\n",
    "                'val_loss': f'{avg_val_loss:.4f}', \n",
    "                'val_acc': f'{val_accuracy:.2f}%'\n",
    "            })\n",
    "    \n",
    "    # Calculate average loss, accuracy, and ROC_AUC Score\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    auc = roc_auc_score(\n",
    "        np.concatenate(all_labels), \n",
    "        np.concatenate(all_preds)\n",
    "    )\n",
    "\n",
    "    # Log validation metrics\n",
    "    if wandb_run:\n",
    "        log_dict = {\n",
    "            f'{phase.lower()}_loss': avg_val_loss,\n",
    "            f'{phase.lower()}_accuracy': val_accuracy,\n",
    "            f'{phase.lower()}_auc': auc,\n",
    "            'epoch': epoch + 1,\n",
    "            'phase': phase\n",
    "        }\n",
    "        wandb_run.log(log_dict)\n",
    "    \n",
    "    # Return average validation loss and accuracy\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1eb25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbad3cc",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "Fine-tuning is a critical technique in transfer learning where we take a pre-trained model and selectively retrain some of its layers to adapt it to our specific task. The `fine_tuning()` function implements this technique for skin cancer classification by:\n",
    "\n",
    "1. **Selective Layer Freezing**: The function freezes early layers of the Xception model that have learned general visual features, while unfreezing later layers (blocks 11, 12, conv3, conv4, and fc) to allow adaptation to skin lesion characteristics.\n",
    "\n",
    "2. **Differential Learning Rates**: The function applies smaller learning rates to the middle layers and a higher learning rate (10x) to the final classification layer, enabling fine-grained parameter updates according to each layer's role.\n",
    "\n",
    "3. **Early Stopping**: Training automatically stops when validation loss fails to improve for a specified number of epochs, preventing overfitting and saving computational resources.\n",
    "\n",
    "This approach leverages the general feature extraction capabilities already learned by the model from ImageNet while specializing the deeper layers for distinguishing between benign and malignant skin lesions, achieving better performance than either training from scratch or using the pre-trained model without adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ffdce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, criterion: nn.Module, epochs: int = 5, patience: int = 5, learning_rate: float = 1e-4, wandb_run: Run = None) -> None:\n",
    "    \"\"\"\n",
    "    Fine-tune the model on the training dataset with early stopping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The pre-trained model to fine-tune.\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for the training dataset.\n",
    "    val_loader : DataLoader\n",
    "        DataLoader for the validation dataset.\n",
    "    criterion : nn.Module\n",
    "        Loss function to compute the loss.\n",
    "    epochs : int, optional\n",
    "        Number of epochs for fine-tuning. Defaults to 10.\n",
    "    patience : int, optional\n",
    "        Number of epochs with no improvement after which training will be stopped. Defaults to 5.\n",
    "    learning_rate : float, optional\n",
    "        Learning rate for the optimizer. Defaults to 1e-4.\n",
    "    wandb_run : Run, optional\n",
    "        Weights & Biases run object for logging metrics. Defaults to None.\n",
    "    \"\"\"\n",
    "    # Initialise the best validation loss and counter for fine-tuning\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # Freeze all layers except the last few blocks for fine-tuning\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(layer in name for layer in ['block11', 'block12', 'conv3', 'conv4', 'fc']):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Define a new optimizer for fine-tuning\n",
    "    optimiser = torch.optim.Adam([\n",
    "        {\"params\": model.block11.parameters(), \"lr\": learning_rate},\n",
    "        {\"params\": model.block12.parameters(), \"lr\": learning_rate},\n",
    "        {\"params\": model.conv3.parameters(), \"lr\": learning_rate},\n",
    "        {\"params\": model.conv4.parameters(), \"lr\": learning_rate},\n",
    "        {\"params\": model.fc.parameters(), \"lr\": learning_rate * 10}\n",
    "    ])\n",
    "\n",
    "    # Fine-tuning loop with early stopping\n",
    "    for epoch in range(epochs):\n",
    "        _, _ = train_epoch(model, train_loader, criterion, optimiser, epochs, wandb_run=wandb_run, current_epoch=epoch, phase='Fine-tuning')\n",
    "        val_loss, _ = validate_epoch(model, val_loader, criterion, epochs, wandb_run=wandb_run, phase='Fine-tuning Validation')\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05011bfb",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "The `train_model()` function orchestrates the entire training process for skin cancer classification models. This function:\n",
    "\n",
    "1. **Model Preparation**:\n",
    "  - Moves the model to the appropriate device (CPU/GPU)\n",
    "  - Initializes the Adam optimizer with the specified learning rate\n",
    "\n",
    "2. **Loss Function Configuration**:\n",
    "  - Optionally applies class weighting to address dataset imbalance\n",
    "  - Uses CrossEntropyLoss or can be configured to use FocalLoss\n",
    "\n",
    "3. **Training Process Management**:\n",
    "  - Implements early stopping to prevent overfitting\n",
    "  - Tracks best validation performance\n",
    "  - Logs metrics to Weights & Biases for experiment tracking\n",
    "\n",
    "4. **Fine-tuning Support**:\n",
    "  - Optionally enables fine-tuning after initial training\n",
    "  - Uses a selective layer unfreezing approach with differential learning rates\n",
    "\n",
    "The function provides a comprehensive framework for training deep learning models on skin lesion datasets, with flexibility for experimenting with various training strategies and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "212905c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 5, learning_rate: float = 1e-3, fine_tune: bool = False, use_class_weights: bool = True, wandb_run: Run = None) -> None:\n",
    "    \"\"\"\n",
    "    Train the model with feature extraction and optional fine-tuning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The model to train.\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for the training dataset.\n",
    "    val_loader : DataLoader\n",
    "        DataLoader for the validation dataset.\n",
    "    epochs : int, optional\n",
    "        Number of epochs to train the model. Defaults to 5.\n",
    "    learning_rate : float, optional\n",
    "        Learning rate for the optimizer. Defaults to 1e-3.\n",
    "    fine_tune : bool, optional\n",
    "        Whether to fine-tune the model after feature extraction. Defaults to False.\n",
    "    use_class_weights : bool, optional\n",
    "        Whether to use class weights in the loss function. Defaults to True.\n",
    "    wandb_run : Run, optional\n",
    "        Weights & Biases run object for logging metrics. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the device for training\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialise the optimiser\n",
    "    optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Set up early stopping parameters\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # Check if class weights should be used\n",
    "    if use_class_weights:\n",
    "        # Extract labels from the train_loader\n",
    "        all_labels = []\n",
    "        for _, labels in train_loader:\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "        train_labels = np.concatenate(all_labels)\n",
    "\n",
    "        # Compute class weights using the labels\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "        # Define the loss function with class weights\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    else:\n",
    "        # Define the loss function without class weights\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # criterion = FocalLoss(alpha=1.0, gamma=2.0, reduction='mean')\n",
    "    \n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    if fine_tune:\n",
    "        # Unfreeze only the classifier\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        _, _ = train_epoch(model, train_loader, criterion, optimiser, epochs, current_epoch=epoch, wandb_run=wandb_run)\n",
    "        val_loss, _ = validate_epoch(model, val_loader, criterion, epochs, wandb_run=wandb_run)  \n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    # Fine-tuning the model\n",
    "    if fine_tune:\n",
    "        print('\\n' + '#' * 50)\n",
    "        print(\"Starting fine-tuning...\")\n",
    "        print('#' * 50, '\\n')\n",
    "\n",
    "        fine_tuning(\n",
    "            model, \n",
    "            train_loader, \n",
    "            val_loader, \n",
    "            criterion, \n",
    "            epochs=epochs, \n",
    "            patience=patience, \n",
    "            learning_rate=learning_rate, \n",
    "            wandb_run=wandb_run\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6ce98",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "The `save_model()` function persists trained deep learning models to disk for future use. It ensures that models are preserved after training, allowing for later inference without retraining. The function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a513ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(model: nn.Module, model_dir: str, model_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the trained model to the specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The trained model to save.\n",
    "    model_dir : str\n",
    "        Directory where the model will be saved.\n",
    "    model_name : str\n",
    "        Name of the model file to save (e.g., 'model.pth').\n",
    "    \"\"\"\n",
    "    # Ensure the model directory exists\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_path = Path(model_dir) / model_name\n",
    "\n",
    "    # Save the model state dictionary\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27557a5",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "The `load_model()` function restores a previously trained model from disk for inference or further training. This enables model reuse without retraining, which is especially valuable for computationally expensive deep learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4db06b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model: nn.Module, model_path: str) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Load the pre-trained model weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The model architecture to load weights into.\n",
    "    model_path : str\n",
    "        Path to the saved model weights.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    nn.Module\n",
    "        The model with loaded weights.\n",
    "    \"\"\"\n",
    "    # Check if the model path exists\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "    \n",
    "    # Load the model state dictionary\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    # Move the model to the appropriate device and return\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241a4ee",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "The `test_model()` function evaluates the performance of a trained deep learning model on unseen data. This function:\n",
    "\n",
    "1. **Performs Inference on Test Data**:\n",
    "  - Sets the model to evaluation mode to disable dropout and batch normalisation updates\n",
    "  - Processes the test dataset batch by batch\n",
    "  - Collects model predictions and ground truth labels\n",
    "\n",
    "2. **Calculates Performance Metrics**:\n",
    "  - Classification report with precision, recall, and F1-score for each class\n",
    "  - Confusion matrix to visualise true positives, false positives, true negatives, and false negatives\n",
    "  - ROC AUC score to evaluate the model's discriminative ability\n",
    "\n",
    "3. **Visualization and Reporting**:\n",
    "  - Generates a visual heatmap of the confusion matrix\n",
    "  - Formats and prints classification metrics for easy interpretation\n",
    "\n",
    "4. **Optional Experiment Tracking**:\n",
    "  - When provided with a Weights & Biases run object, logs all metrics and visualisations\n",
    "  - Supports detailed experiment comparison and model versioning\n",
    "\n",
    "This comprehensive evaluation helps determine the model's effectiveness at identifying malignant skin lesions and provides insights for potential improvements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f47093bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: nn.Module, test_loader: DataLoader, wandb_run: Run = None) -> None:\n",
    "    \"\"\"\n",
    "    Test the model on the test dataset and log results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The trained model to evaluate.\n",
    "    test_loader : DataLoader\n",
    "        DataLoader for the test dataset.\n",
    "    wandb_run : Run, optional\n",
    "        Weights & Biases run object for logging. Defaults to None.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store predictions and labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Loop through the test set\n",
    "        for images, labels in test_loader:\n",
    "            # Move data to device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Classification report\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Benign', 'Malignant']))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Compute ROC_AUC Score\n",
    "    auc = roc_auc_score(all_labels, all_preds) \n",
    "    print('ROC AUC Score:', auc)\n",
    "\n",
    "    # Log confusion matrix to Weights & Biases\n",
    "    if wandb_run:\n",
    "        wandb_run.log({\n",
    "            'classification_report': wandb.Table(dataframe=pd.DataFrame(classification_report(all_labels, all_preds, target_names=['Benign', 'Malignant'], output_dict=True)).transpose()),\n",
    "            'confusion_matrix': wandb.plot.confusion_matrix(\n",
    "                probs=None,\n",
    "                y_true=all_labels,\n",
    "                preds=all_preds,\n",
    "                class_names=['Benign', 'Malignant']\n",
    "            ),\n",
    "            'accuracy': np.mean(np.array(all_preds) == np.array(all_labels)),\n",
    "            'roc_auc_score': auc\n",
    "        })\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Benign', 'Malignant'],\n",
    "                yticklabels=['Benign', 'Malignant'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd5c1b",
   "metadata": {},
   "source": [
    "## Create an Experiment\n",
    "\n",
    "The `create_wandb_experiment()` function integrates our skin cancer detection model with Weights & Biases (W&B) for experiment tracking. This function:\n",
    "\n",
    "1. **Initialises W&B Experiment**: Creates a new experiment run with the provided name\n",
    "2. **Configures Experiment Parameters**: Registers hyperparameters and settings in the W&B dashboard\n",
    "3. **Establishes Project Context**: Links the experiment to our skin cancer detection project\n",
    "4. **Handles Authentication**: Uses the API key from environment variables to access W&B services\n",
    "5. **Returns Control Object**: Provides a run object for logging metrics throughout model training\n",
    "\n",
    "This integration enables systematic tracking of model performance, visualisation of results, and comparison of different experimental configurations, which is essential for methodical research in deep learning applications for medical image analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61aeea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wandb_experiment(name:str, config: dict) -> Run:\n",
    "    \"\"\"\n",
    "    Function to create experiment to log the model and metrics to Weights & Biases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Name of the experiment run.\n",
    "    config : dict\n",
    "        Configuration dictionary containing hyperparameters and other settings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    wandb.run : Run\n",
    "        A Weights & Biases run object for logging metrics and artifacts.\n",
    "    \"\"\"\n",
    "\n",
    "    return wandb.init(\n",
    "        # Set the wandb entity and project name\n",
    "        entity=WANDB_ENTITY,\n",
    "        project=WANDB_PROJECT,\n",
    "\n",
    "        # Set the name of the run\n",
    "        name=name,\n",
    "\n",
    "        # Set the configuration for the run\n",
    "        config=config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b9b75",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "The `experiment()` function allows us to systematically run experiments with different configurations of our skin cancer detection model. Using this function, we can:\n",
    "\n",
    "1. **Explore Different Data Augmentation Techniques**\n",
    "  - Control image transformations such as random flips, affine transformations, and color adjustments\n",
    "  - Evaluate the impact of augmentation strategies on model performance\n",
    "\n",
    "2. **Optimise Hyperparameters**\n",
    "  - Experiment with learning rates, batch sizes, and training epochs\n",
    "  - Test the effects of class weighting for handling dataset imbalance\n",
    "\n",
    "3. **Compare Training Approaches**\n",
    "  - Evaluate the impact of fine-tuning versus feature extraction\n",
    "  - Assess different model architectures for skin cancer classification\n",
    "\n",
    "4. **Track Performance Metrics**\n",
    "  - All experiments are logged to Weights & Biases for visualisation and comparison\n",
    "  - Performance is evaluated using accuracy, loss, and ROC AUC scores\n",
    "\n",
    "This systematic experimentation approach helps identify the optimal model configuration for skin cancer detection, balancing accuracy, generalisation, and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aff05f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model: nn.Module, data_loaders: tuple, hyperparameters: dict = {}, experiment_name: str = 'experiment', save_model: bool = False, save_model_as: str = 'model.pth') -> None:\n",
    "    \"\"\"\n",
    "    Main function to run the skin cancer classification experiment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Model to experiment with\n",
    "    data_loaders : tuple\n",
    "        Tuple containing DataLoader objects for training, validation, and testing datasets.\n",
    "    hyperparameters : dict, optional\n",
    "        Dictionary containing hyperparameters for training. Defaults to an empty dictionary.\n",
    "    experiment_name : str, optional\n",
    "        Name of the experiment for logging purposes. Defaults to 'experiment'.\n",
    "    save_model : bool, optional\n",
    "        Whether to save the trained model. Defaults to False.\n",
    "    save_model_as : str, optional\n",
    "        Name of the file to save the model as. Defaults to 'model.pth'.\n",
    "    \"\"\"\n",
    "    # Create a Weights & Biases experiment\n",
    "    run = create_wandb_experiment(\n",
    "        name=experiment_name,\n",
    "        config=hyperparameters\n",
    "    )\n",
    "\n",
    "    # Unpack the data loaders\n",
    "    train_loader, val_loader, test_loader = data_loaders\n",
    "\n",
    "    # Train the model\n",
    "    print('#' * 50)\n",
    "    print(\"Training the model...\")\n",
    "    print('#' * 50, '\\n')\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=hyperparameters['epochs'],\n",
    "        learning_rate=hyperparameters['learning_rate'],\n",
    "        use_class_weights=hyperparameters['use_class_weights'],\n",
    "        fine_tune=hyperparameters['fine_tune'],\n",
    "        wandb_run=run,\n",
    "    )\n",
    "\n",
    "    # Test the model\n",
    "    print('\\n\\n' + '#' * 50)\n",
    "    print(\"Testing the model...\")\n",
    "    print('#' * 50, '\\n')\n",
    "    test_model(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        wandb_run=run\n",
    "    )\n",
    "\n",
    "    # Save the model\n",
    "    if save_model:\n",
    "        print('#' * 50)\n",
    "        print(\"Saving the model...\\n\")\n",
    "        print('#' * 50, '\\n')\n",
    "        save_model_weights(\n",
    "            model=model,\n",
    "            model_dir=model_dir,\n",
    "            model_name=save_model_as\n",
    "        )\n",
    "    \n",
    "    # Finish the Weights & Biases run\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d99ec5",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "# Model Architecture Analysis\n",
    "\n",
    "In this section, we analyse the architecture of our pre-trained models for skin cancer detection with various configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64202f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mavishekdangol\u001b[0m (\u001b[33mthe_lab\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dango\\OneDrive - UWE Bristol\\projects\\thesis-project\\notebooks\\wandb\\run-20250714_142458-0zuyyfio</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/the_lab/skin_cancer_detection/runs/0zuyyfio' target=\"_blank\">ViT-class_weights</a></strong> to <a href='https://wandb.ai/the_lab/skin_cancer_detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/the_lab/skin_cancer_detection' target=\"_blank\">https://wandb.ai/the_lab/skin_cancer_detection</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/the_lab/skin_cancer_detection/runs/0zuyyfio' target=\"_blank\">https://wandb.ai/the_lab/skin_cancer_detection/runs/0zuyyfio</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "Training the model...\n",
      "################################################## \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/5]: 100%|█████████████████████████████████████████████| 50/50 [13:49<00:00, 16.59s/it, loss=0.4517, acc=79.45%]\n",
      "Validation: 100%|████████████████████████████████████████| 5/5 [00:26<00:00,  5.33s/it, val_loss=0.7277, val_acc=78.63%]\n",
      "Epoch [2/5]: 100%|█████████████████████████████████████████████| 50/50 [15:53<00:00, 19.07s/it, loss=0.0692, acc=98.28%]\n",
      "Validation: 100%|████████████████████████████████████████| 5/5 [00:29<00:00,  5.85s/it, val_loss=0.8215, val_acc=81.68%]\n",
      "Epoch [3/5]: 100%|█████████████████████████████████████████████| 50/50 [14:35<00:00, 17.51s/it, loss=0.0511, acc=98.41%]\n",
      "Validation: 100%|████████████████████████████████████████| 5/5 [00:23<00:00,  4.77s/it, val_loss=0.8051, val_acc=79.39%]\n",
      "Epoch [4/5]:  42%|██████████████████▉                          | 21/50 [05:31<07:51, 16.24s/it, loss=0.1174, acc=95.98%]"
     ]
    }
   ],
   "source": [
    "model = get_model('ViT')\n",
    "\n",
    "# Initialise the constants\n",
    "experiment_name = 'ViT-class_weights'\n",
    "\n",
    "hyperparameters = {\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 1e-4,\n",
    "    'batch_size': batch_size,\n",
    "    'use_class_weights': True,\n",
    "    'fine_tune': False\n",
    "}\n",
    "\n",
    "# Perform the experiment\n",
    "experiment(\n",
    "    model=model,\n",
    "    data_loaders=(train_loader, val_loader, test_loader),\n",
    "    hyperparameters=hyperparameters,\n",
    "    experiment_name=experiment_name,\n",
    "    save_model=True,\n",
    "    save_model_as=f'{experiment_name}.pth'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
