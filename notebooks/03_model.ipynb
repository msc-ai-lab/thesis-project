{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eb5cd6",
   "metadata": {},
   "source": [
    "# Skin Cancer Detection using Deep Learning\n",
    "This Jupyter notebook demonstrates how to build a deep learning model for skin cancer detection using the ResNet architecture. The model classifies skin lesion images as either benign or malignant.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Please run `02_data_preparation.ipynb` before running this notebook as it uses the output of data preparation. It ensures that the processed data are located where it is needed for this notebook to run smoothly.\n",
    "\n",
    "### Dataset Requirements\n",
    "- The processed dataset from `02_data_preparation.ipynb` must be inside `data/processed` folder\n",
    "- Expected structure:\n",
    "  - `data/processed/train_dataset.pt` - Processed training dataset\n",
    "  - `data/processed/val_dataset.pt` - Processed validation dataset\n",
    "  - `data/processed/test_dataset.pt` - Processed testing dataset\n",
    "\n",
    "### Experiments Requirements\n",
    "This project utilises Weights & Biases (wandb) to experiment various model configurations. To perform experiments with different configurations, you need to follow the following steps first:\n",
    "\n",
    "- Create a wandb account at [wandb.ai](https://wandb.ai) if you don't have one yet\n",
    "- Get your Wandb API key from your account settings\n",
    "- Add your Wandb API key in the `.env` file with the key `WANDB_API_KEY`\n",
    "- If the `.env` file doesn't exist, copy the `.env.example` file to create `.env` and replace the placeholder with your actual API key\n",
    "\n",
    "### Environment Setup\n",
    "Install the required packages from `requirements.txt` file.\n",
    "\n",
    "### Notebook Structure\n",
    "This notebook is organised into the following sections:\n",
    "- Import Libraries - Required Python packages for the project\n",
    "- Constants - Define paths and other constants\n",
    "- Get Environment Variables - Load configuration for Weights & Biases\n",
    "- Load Datasets - Prepare processed datasets for model training\n",
    "- Train the Model - Functions for model training, validation, and fine-tuning\n",
    "- Save the Model - Utilities to persist trained models\n",
    "- Test the Model - Evaluate model performance\n",
    "- Create an Experiment - Set up Weights & Biases experiment tracking\n",
    "- Experiments - Run training with specific configurations\n",
    "- Model Architecture Analysis - Analyse different model architectures\n",
    "- Hyperparameters Tuning - Optimise model performance through systematic hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a25985",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softmax\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import yaml\n",
    "import wandb\n",
    "from wandb.sdk.wandb_run import Run\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local imports\n",
    "from scd.utils.common import load_datasets\n",
    "from scd.model import SkinCancerCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbe63bb",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a73123",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2 # Malignant and Benign\n",
    "random_state = 42\n",
    "\n",
    "# Define paths\n",
    "root_dir = Path.cwd().parent\n",
    "model_dir = root_dir / 'models'\n",
    "processed_data_dir = root_dir / 'data' / 'processed'\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dd2d8a",
   "metadata": {},
   "source": [
    "## Get Environment Variables\n",
    "\n",
    "We load environment variables from the `.env` file, which contains configuration for Weights & Biases (W&B) experiments.\n",
    "\n",
    "The following environment variables are loaded:\n",
    "- `WANDB_API_KEY`: Authentication key for accessing W&B services\n",
    "- `WANDB_ENTITY`: Username or organization name in W&B\n",
    "- `WANDB_PROJECT`: Name of the project in W&B for organizing experiments\n",
    "\n",
    "These configurations enable systematic tracking of our skin cancer detection experiments, allowing us to compare different model architectures and hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f45509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "WANDB_API_KEY = os.getenv('WANDB_API_KEY')\n",
    "WANDB_ENTITY = os.getenv('WANDB_ENTITY') or 'the_lab'\n",
    "WANDB_PROJECT = os.getenv('WANDB_PROJECT') or 'skin_cancer_detection'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68e450",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "\n",
    "We load the pre-processed datasets that were prepared in the previous notebook. These datasets contain skin lesion images that have been resized to the appropriate dimensions for our ResNet34 model (384x384 pixels) and normalised according to ImageNet statistics.\n",
    "\n",
    "The datasets are loaded as TensorDatasets, which contain both the image tensors and their corresponding labels (benign or malignant). We then create DataLoader objects to efficiently batch the data during training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4268802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Load the saved datasets\n",
    "train_tensor_dataset, val_tensor_dataset, test_tensor_dataset = load_datasets(processed_data_dir)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_tensor_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_tensor_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d760451",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68fdaeb",
   "metadata": {},
   "source": [
    "### Train Epoch\n",
    "\n",
    "The `train_epoch` function handles one complete training cycle through all batches in the training dataset. It tracks key metrics and reports them to Weights & Biases if an experiment run is provided for logging, enabling experiment tracking and visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e640903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model: nn.Module, train_loader: DataLoader, criterion: nn.Module, optimiser: torch.optim.Optimizer, epochs: int, current_epoch: int, wandb_run: Run = None, phase: str = 'Training') -> tuple:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The model to train.\n",
    "    train_loader : DataLoader\n",
    "        The data loader for the training data.\n",
    "    criterion : nn.Module\n",
    "        The loss function.\n",
    "    optimiser : torch.optim.Optimizer\n",
    "        The optimizer for updating model weights.\n",
    "    epochs : int\n",
    "        The total number of epochs for training.\n",
    "    current_epoch : int\n",
    "        The current epoch number.\n",
    "    wandb_run : Run, optional\n",
    "        The Weights & Biases run object for logging.\n",
    "    phase : str, optional\n",
    "        The phase of training (default is 'Training').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        The average loss and accuracy for the epoch.\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch [{current_epoch+1}/{epochs}]\", ncols=120)\n",
    "\n",
    "    # Training loop\n",
    "    for i, (images, labels) in enumerate(train_pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        if isinstance(outputs, tuple):\n",
    "            outputs = outputs[0]\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        mean_loss = running_loss / (i + 1)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        train_pbar.set_postfix({\n",
    "            'loss': f'{mean_loss:.4f}',\n",
    "            'acc': f'{accuracy:.2f}%',\n",
    "        })\n",
    "\n",
    "    # Log once per epoch\n",
    "    if wandb_run:\n",
    "        wandb_run.log({\n",
    "            'train_loss': mean_loss,\n",
    "            'train_accuracy': accuracy,\n",
    "            'epoch': current_epoch + 1,\n",
    "            'phase': phase\n",
    "        })\n",
    "\n",
    "    return mean_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12445bd",
   "metadata": {},
   "source": [
    "### Validate Epoch\n",
    "\n",
    "The `validate_epoch` function evaluates the model's performance on validation data during training. It measures metrics like loss, accuracy, and ROC AUC score to track how well the model generalizes to unseen data. This function is crucial for monitoring model performance, detecting overfitting, and determining when to stop training or adjust hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac60905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(model: nn.Module, val_loader: DataLoader, criterion: nn.Module, epoch: int, wandb_run: Run = None, phase: str = 'Validation') -> tuple:\n",
    "    \"\"\"\n",
    "    Validate the model for one epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The model to validate.\n",
    "    val_loader : DataLoader\n",
    "        The data loader for the validation data.\n",
    "    criterion : nn.Module\n",
    "        The loss function.\n",
    "    epoch : int\n",
    "        The current epoch number.\n",
    "    wandb_run : Run, optional\n",
    "        The Weights & Biases run object for logging.\n",
    "    phase : str, optional\n",
    "        The phase of validation (default is 'Validation').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        The average loss and accuracy for the epoch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    val_pbar = tqdm(val_loader, desc=phase, ncols=120)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs, _ = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            probs = softmax(outputs, dim=1)  # or F.sigmoid for binary\n",
    "            _, predicted = torch.max(probs, 1)\n",
    "\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "\n",
    "            val_pbar.set_postfix({\n",
    "                'val_loss': f'{val_loss / (val_total // labels.size(0)):.4f}',\n",
    "                'val_acc': f'{100 * val_correct / val_total:.2f}%'\n",
    "            })\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    # Flatten arrays\n",
    "    y_true = np.concatenate(all_labels)\n",
    "    y_score = np.concatenate(all_probs)\n",
    "\n",
    "    # Calculate AUC\n",
    "    auc = roc_auc_score(y_true, y_score[:, 1])\n",
    "\n",
    "    if wandb_run:\n",
    "        wandb_run.log({\n",
    "            f'{phase.lower()}_loss': avg_val_loss,\n",
    "            f'{phase.lower()}_accuracy': val_accuracy,\n",
    "            f'{phase.lower()}_auc': auc,\n",
    "            'epoch': epoch + 1,\n",
    "            'phase': phase\n",
    "        })\n",
    "\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbad3cc",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "Fine-tuning is a critical technique in transfer learning where we take a pre-trained model and selectively retrain some of its layers to adapt it to our specific task. The `fine_tuning()` function implements this technique for skin cancer classification by:\n",
    "\n",
    "1. **Selective Layer Freezing**: The function freezes early layers of the model that have learned general visual features, while unfreezing later layers to allow adaptation to skin lesion characteristics.\n",
    "\n",
    "2. **Differential Learning Rates**: The function applies smaller learning rates to the middle layers and a higher learning rate (5x) to the final classification layer, enabling fine-grained parameter updates according to each layer's role.\n",
    "\n",
    "This approach leverages the general feature extraction capabilities already learned by the model while specialising the deeper layers for distinguishing between benign and malignant skin lesions, achieving better performance than either training from scratch or using the pre-trained model without adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffdce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, criterion: nn.Module, epochs: int = 5, patience: int = 5, learning_rate: float = 1e-4, wandb_run: Run = None) -> None:\n",
    "    \"\"\"\n",
    "    Fine-tune the model on the training dataset with early stopping.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The pre-trained model to fine-tune.\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for the training dataset.\n",
    "    val_loader : DataLoader\n",
    "        DataLoader for the validation dataset.\n",
    "    criterion : nn.Module\n",
    "        Loss function to compute the loss.\n",
    "    epochs : int, optional\n",
    "        Number of epochs for fine-tuning. Defaults to 10.\n",
    "    patience : int, optional\n",
    "        Number of epochs with no improvement after which training will be stopped. Defaults to 5.\n",
    "    learning_rate : float, optional\n",
    "        Learning rate for the optimizer. Defaults to 1e-4.\n",
    "    wandb_run : Run, optional\n",
    "        Weights & Biases run object for logging metrics. Defaults to None.\n",
    "    \"\"\"\n",
    "    # Initialise the best validation loss and counter for fine-tuning\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # Unfreeze deeper layers for fine-tuning\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(layer in name for layer in ['backbone.layer4', 'attention', 'classifier']):\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Define optimiser\n",
    "    optimiser = torch.optim.Adam([\n",
    "        {\"params\": model.backbone.layer4.parameters(), \"lr\": learning_rate},\n",
    "        {\"params\": model.attention.parameters(), \"lr\": learning_rate},\n",
    "        {\"params\": model.classifier.parameters(), \"lr\": learning_rate * 5}\n",
    "    ])\n",
    "\n",
    "    # Fine-tuning loop with early stopping\n",
    "    for epoch in range(epochs):\n",
    "        _, _ = train_epoch(model, train_loader, criterion, optimiser, epochs, current_epoch=epoch, wandb_run=wandb_run, phase='Fine-tuning')\n",
    "        val_loss, _ = validate_epoch(model, val_loader, criterion, epoch, wandb_run=wandb_run, phase='Fine-tuning Validation')\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05011bfb",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "\n",
    "The `train_model()` function orchestrates the entire training process for skin cancer classification models. This function:\n",
    "\n",
    "1. **Model Preparation**:\n",
    "  - Moves the model to the appropriate device (CPU/GPU)\n",
    "  - Initialises the Adam optimizer with the specified learning rate\n",
    "\n",
    "2. **Loss Function Configuration**:\n",
    "  - Optionally applies class weighting to address dataset imbalance\n",
    "\n",
    "3. **Training Process Management**:\n",
    "  - Implements early stopping to prevent overfitting\n",
    "  - Tracks best validation performance\n",
    "  - Logs metrics to Weights & Biases for experiment tracking\n",
    "\n",
    "4. **Fine-tuning Support**:\n",
    "  - Optionally enables fine-tuning after initial training\n",
    "  - Uses a selective layer unfreezing approach with differential learning rates\n",
    "\n",
    "The function provides a comprehensive framework for training deep learning models on skin lesion datasets, with flexibility for experimenting with various training strategies and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212905c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 5, learning_rate: float = 1e-3, fine_tune: bool = False, use_class_weights: bool = True, wandb_run: Run = None) -> None:\n",
    "    \"\"\"\n",
    "    Train the model with feature extraction and optional fine-tuning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The model to train.\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for the training dataset.\n",
    "    val_loader : DataLoader\n",
    "        DataLoader for the validation dataset.\n",
    "    epochs : int, optional\n",
    "        Number of epochs to train the model. Defaults to 5.\n",
    "    learning_rate : float, optional\n",
    "        Learning rate for the optimizer. Defaults to 1e-3.\n",
    "    fine_tune : bool, optional\n",
    "        Whether to fine-tune the model after feature extraction. Defaults to False.\n",
    "    use_class_weights : bool, optional\n",
    "        Whether to use class weights in the loss function. Defaults to True.\n",
    "    wandb_run : Run, optional\n",
    "        Weights & Biases run object for logging metrics. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the device for training\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialise the optimiser\n",
    "    optimiser = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Set up early stopping parameters\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # Check if class weights should be used\n",
    "    if use_class_weights:\n",
    "        # Extract labels from the train_loader\n",
    "        all_labels = []\n",
    "        for _, labels in train_loader:\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "        train_labels = np.concatenate(all_labels)\n",
    "\n",
    "        # Compute class weights using the labels\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "        # Define the loss function with class weights\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    else:\n",
    "        # Define the loss function without class weights\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    if fine_tune:\n",
    "        # Freeze the pretrained backbone layers\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'features' in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        _, _ = train_epoch(model, train_loader, criterion, optimiser, epochs, current_epoch=epoch, wandb_run=wandb_run)\n",
    "        val_loss, _ = validate_epoch(model, val_loader, criterion, epoch, wandb_run=wandb_run)\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    # Fine-tuning the model\n",
    "    if fine_tune:\n",
    "        print('\\n' + '#' * 50)\n",
    "        print(\"Starting fine-tuning...\")\n",
    "        print('#' * 50, '\\n')\n",
    "\n",
    "        fine_tuning(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            criterion,\n",
    "            epochs=epochs,\n",
    "            patience=patience,\n",
    "            learning_rate=learning_rate,\n",
    "            wandb_run=wandb_run\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6ce98",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "The `save_model()` function persists trained deep learning models to disk for future use. It ensures that models are preserved after training, allowing for later inference without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a513ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_weights(model: nn.Module, model_dir: str, model_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the trained model to the specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The trained model to save.\n",
    "    model_dir : str\n",
    "        Directory where the model will be saved.\n",
    "    model_name : str\n",
    "        Name of the model file to save (e.g., 'model.pth').\n",
    "    \"\"\"\n",
    "    # Ensure the model directory exists\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_path = Path(model_dir) / model_name\n",
    "\n",
    "    # Save the model state dictionary\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241a4ee",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "The `test_model()` function evaluates the performance of a trained deep learning model on unseen data. This function:\n",
    "\n",
    "1. **Performs Inference on Test Data**:\n",
    "  - Sets the model to evaluation mode to disable dropout and batch normalisation updates\n",
    "  - Processes the test dataset batch by batch\n",
    "  - Collects model predictions and ground truth labels\n",
    "\n",
    "2. **Calculates Performance Metrics**:\n",
    "  - Classification report with precision, recall, and F1-score for each class\n",
    "  - Confusion matrix to visualise true positives, false positives, true negatives, and false negatives\n",
    "  - ROC AUC score to evaluate the model's discriminative ability\n",
    "\n",
    "3. **Visualisation and Reporting**:\n",
    "  - Generates a visual heatmap of the confusion matrix\n",
    "  - Formats and prints classification metrics for easy interpretation\n",
    "\n",
    "4. **Optional Experiment Tracking**:\n",
    "  - When provided with a Weights & Biases run object, logs all metrics and visualisations\n",
    "  - Supports detailed experiment comparison and model versioning\n",
    "\n",
    "This comprehensive evaluation helps determine the model's effectiveness at identifying malignant skin lesions and provides insights for potential improvements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47093bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: nn.Module, test_loader: DataLoader, wandb_run: Run = None) -> None:\n",
    "    \"\"\"\n",
    "    Test the model on the test dataset and log results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The trained model to evaluate.\n",
    "    test_loader : DataLoader\n",
    "        DataLoader for the test dataset.\n",
    "    wandb_run : Run, optional\n",
    "        Weights & Biases run object for logging. Defaults to None.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store predictions and labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Loop through the test set\n",
    "        for images, labels in test_loader:\n",
    "            # Move data to device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Predict using the model\n",
    "            outputs, _ = model(images)\n",
    "\n",
    "            # Get predictions\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Classification report\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Benign', 'Malignant']))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Compute ROC_AUC Score\n",
    "    auc = roc_auc_score(all_labels, all_preds) \n",
    "    print('ROC AUC Score:', auc)\n",
    "\n",
    "    # Log confusion matrix to Weights & Biases\n",
    "    if wandb_run:\n",
    "        wandb_run.log({\n",
    "            'classification_report': wandb.Table(dataframe=pd.DataFrame(classification_report(all_labels, all_preds, target_names=['Benign', 'Malignant'], output_dict=True)).transpose()),\n",
    "            'confusion_matrix': wandb.plot.confusion_matrix(\n",
    "                probs=None,\n",
    "                y_true=all_labels,\n",
    "                preds=all_preds,\n",
    "                class_names=['Benign', 'Malignant']\n",
    "            ),\n",
    "            'accuracy': np.mean(np.array(all_preds) == np.array(all_labels)) *100,\n",
    "            'roc_auc_score': auc\n",
    "        })\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Benign', 'Malignant'],\n",
    "                yticklabels=['Benign', 'Malignant'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd5c1b",
   "metadata": {},
   "source": [
    "## Create an Experiment\n",
    "\n",
    "The `create_wandb_experiment()` function integrates our skin cancer detection model with Weights & Biases (W&B) for experiment tracking. This function:\n",
    "\n",
    "1. **Initialises W&B Experiment**: Creates a new experiment run with the provided name\n",
    "2. **Configures Experiment Parameters**: Registers hyperparameters and settings in the W&B dashboard\n",
    "3. **Establishes Project Context**: Links the experiment to our skin cancer detection project\n",
    "4. **Handles Authentication**: Uses the API key from environment variables to access W&B services\n",
    "5. **Returns Control Object**: Provides a run object for logging metrics throughout model training\n",
    "\n",
    "This integration enables systematic tracking of model performance, visualisation of results, and comparison of different experimental configurations, which is essential for methodical research in deep learning applications for medical image analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aeea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wandb_experiment(name:str, config: dict) -> Run:\n",
    "    \"\"\"\n",
    "    Function to create experiment to log the model and metrics to Weights & Biases.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Name of the experiment run.\n",
    "    config : dict\n",
    "        Configuration dictionary containing hyperparameters and other settings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    wandb.run : Run\n",
    "        A Weights & Biases run object for logging metrics and artifacts.\n",
    "    \"\"\"\n",
    "\n",
    "    return wandb.init(\n",
    "        # Set the wandb entity and project name\n",
    "        entity=WANDB_ENTITY,\n",
    "        project=WANDB_PROJECT,\n",
    "\n",
    "        # Set the name of the run\n",
    "        name=name,\n",
    "\n",
    "        # Set the configuration for the run\n",
    "        config=config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0b9b75",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "The `experiment()` function allows us to systematically run experiments with different configurations of our skin cancer detection model. The systematic experimentation approach helps identify the optimal model configuration for skin cancer detection, balancing accuracy, generalisation, and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff05f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model: nn.Module, data_loaders: tuple, hyperparameters: dict = {}, experiment_name: str = 'experiment', save_model: bool = False, save_model_as: str = 'model.pth') -> None:\n",
    "    \"\"\"\n",
    "    Main function to run the skin cancer classification experiment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Model to experiment with\n",
    "    data_loaders : tuple\n",
    "        Tuple containing DataLoader objects for training, validation, and testing datasets.\n",
    "    hyperparameters : dict, optional\n",
    "        Dictionary containing hyperparameters for training. Defaults to an empty dictionary.\n",
    "    experiment_name : str, optional\n",
    "        Name of the experiment for logging purposes. Defaults to 'experiment'.\n",
    "    save_model : bool, optional\n",
    "        Whether to save the trained model. Defaults to False.\n",
    "    save_model_as : str, optional\n",
    "        Name of the file to save the model as. Defaults to 'model.pth'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a Weights & Biases experiment\n",
    "        run = create_wandb_experiment(\n",
    "            name=experiment_name,\n",
    "            config=hyperparameters\n",
    "        )\n",
    "\n",
    "        # Unpack the data loaders\n",
    "        train_loader, val_loader, test_loader = data_loaders\n",
    "\n",
    "        # Train the model\n",
    "        print('#' * 50)\n",
    "        print(\"Training the model...\")\n",
    "        print('#' * 50, '\\n')\n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            epochs=hyperparameters['epochs'],\n",
    "            learning_rate=hyperparameters['learning_rate'],\n",
    "            use_class_weights=hyperparameters['use_class_weights'],\n",
    "            fine_tune=hyperparameters['fine_tune'],\n",
    "            wandb_run=run,\n",
    "        )\n",
    "\n",
    "        # Test the model\n",
    "        print('\\n\\n' + '#' * 50)\n",
    "        print(\"Testing the model...\")\n",
    "        print('#' * 50, '\\n')\n",
    "        test_model(\n",
    "            model=model,\n",
    "            test_loader=test_loader,\n",
    "            wandb_run=run\n",
    "        )\n",
    "\n",
    "        # Save the model\n",
    "        if save_model:\n",
    "            print('#' * 50)\n",
    "            print(\"Saving the model...\")\n",
    "            print('#' * 50, '\\n')\n",
    "            save_model_weights(\n",
    "                model=model,\n",
    "                model_dir=model_dir,\n",
    "                model_name=save_model_as\n",
    "            )\n",
    "        \n",
    "        # Finish the Weights & Biases run\n",
    "        run.finish()\n",
    "    except Exception as e:\n",
    "        if 'run' in locals():\n",
    "            run.finish()\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d99ec5",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "source": [
    "# Model Architecture Analysis\n",
    "\n",
    "In this section, we analyse the architecture of our pre-trained models for skin cancer detection with various configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64202f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkinCancerCNN()\n",
    "\n",
    "# Initialise the constants\n",
    "experiment_name = f'ResNet_skin_cancer_classification'\n",
    "\n",
    "hyperparameters = {\n",
    "    'epochs': 10,\n",
    "    'learning_rate': 1e-4,\n",
    "    'batch_size': batch_size,\n",
    "    'use_class_weights': True,\n",
    "    'fine_tune': False,\n",
    "    'type': 'aug-training'\n",
    "}\n",
    "\n",
    "# Perform the experiment\n",
    "try:\n",
    "  experiment(\n",
    "        model=model,\n",
    "        data_loaders=(train_loader, val_loader, test_loader),\n",
    "        hyperparameters=hyperparameters,\n",
    "        experiment_name=experiment_name,\n",
    "        save_model=True,\n",
    "        save_model_as=f'{experiment_name}.pth'\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the experiment: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8051d505",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning\n",
    "\n",
    "Hyperparameter tuning is a critical step in optimising models. To systematically explore different hyperparameter combinations to improve model performance, we use Weights & Biases Sweep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5aeb44",
   "metadata": {},
   "source": [
    "## Sweep Configuration\n",
    "\n",
    "We load a predefined sweep configuration from `configs/sweep.yaml` that specifies the hyperparameter search space, including learning rates, batch sizes, and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config_path = root_dir / 'configs' / 'sweep.yaml'\n",
    "with open(sweep_config_path, 'r') as file:\n",
    "    sweep_config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf501f1",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "The `train_sweep()` function initialises a W&B run for each hyperparameter combination, trains the model with those parameters, and logs performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef4751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep():\n",
    "    wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        entity=WANDB_ENTITY,\n",
    "        name=f\"sweep_run\"\n",
    "    )\n",
    "\n",
    "    config = wandb.config\n",
    "\n",
    "    train_model(\n",
    "        model=SkinCancerCNN(),\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=config['epochs'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        use_class_weights=True,\n",
    "        fine_tune=False,\n",
    "        wandb_run=wandb,\n",
    "    )\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d8d94c",
   "metadata": {},
   "source": [
    "## Sweep Agent\n",
    "\n",
    "We run multiple training experiments automatically using the W&B agent, which selects hyperparameter combinations according to the search strategy defined in the sweep configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1983ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run wandb sweep command using the sweep configuration\n",
    "sweep_id = wandb.sweep(sweep_config, project=WANDB_PROJECT)\n",
    "\n",
    "# Start the sweep agent to run the training function\n",
    "wandb.agent(sweep_id, function=train_sweep, count=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
