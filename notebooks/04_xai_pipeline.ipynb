{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35ba11e-2ad2-4cbb-b595-34bc8b8bc8eb",
   "metadata": {},
   "source": [
    "# XAI Pipeline for Skin Cancer Classification\n",
    "This notebook loads the trained Xception model, applies there XAI methods (Grad-CAM, SHAP, Influence Functions), and exports the results in a format ready for the LLM integration pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a3447-fa5f-454c-b70c-32b19cf5c731",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369b6a0-dafd-47b4-9ef9-3b7e8820d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup and Initialization ---\n",
    "import sys\n",
    "import os\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Project root added to path: {project_root}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from captum.attr import LayerGradCam, Occlusion, IntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import the model architecture from our central definition file\n",
    "from src.model_def import SkinCancerViT\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe6cff5-5a8c-45ca-8623-91c502223415",
   "metadata": {},
   "source": [
    "## Define Project Structure and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c80821-a160-46cb-a8dc-81c4e41c895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Define Project Structure and File Paths ---\n",
    "\n",
    "BASE_DIR = project_root\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'models', 'temp', 'ViT-class_weights.pth')\n",
    "SAMPLE_IMAGE_PATH = os.path.join(BASE_DIR, 'user_inputs', 'user_sample1.jpg')\n",
    "PROCESSED_TRAIN_DATA_PATH = os.path.join(BASE_DIR, 'data', 'processed', 'ViT', 'train_dataset.pt')\n",
    "TRAIN_SPLIT_PATH = os.path.join(BASE_DIR, 'data', 'processed', 'train_split.csv')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'results', 'xai_output')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Project Base Directory: {BASE_DIR}\")\n",
    "print(f\"Using Model Path: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d96ef-09d5-4ad7-ae91-55cb5daab1c5",
   "metadata": {},
   "source": [
    "## Model Definition and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16bdb5b-2fc0-4956-9037-9f4f18bda9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Model Definition and Loading (Final, Corrected Version) ---\n",
    "print(\"\\nLoading model...\")\n",
    "\n",
    "# Instantiate the model from our central definition file\n",
    "model = SkinCancerViT(num_classes=2)\n",
    "try:\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    print(f\"Successfully loaded trained model weights.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the model state_dict: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193fefa3-41ce-432a-8d4a-fd0e3f37ef5e",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e02957-3b87-461c-b2ab-d8b543042628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Data Loading and Preprocessing ---\n",
    "\n",
    "print(\"\\nLoading and preprocessing sample image...\")\n",
    "\n",
    "# The model expects 224x224 images\n",
    "preprocess_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "try:\n",
    "    input_image = Image.open(SAMPLE_IMAGE_PATH).convert('RGB')\n",
    "    input_tensor = preprocess_transform(input_image).unsqueeze(0)\n",
    "    print(f\"Successfully loaded sample image: {SAMPLE_IMAGE_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Sample image not found at '{SAMPLE_IMAGE_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e79fee-f6b3-4a19-95e2-50623014ebc9",
   "metadata": {},
   "source": [
    "## XAI Pipeline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9ab77-681c-4509-8371-2d4500e66eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. XAI Pipeline Implementation ---\n",
    "\n",
    "print(\"\\n--- Running XAI Pipeline ---\")\n",
    "\n",
    "# Get model prediction\n",
    "logits = model(input_tensor).logits\n",
    "output_softmax = F.softmax(logits, dim=1)\n",
    "prediction_score, pred_label_idx = torch.topk(output_softmax, 1)\n",
    "predicted_class_index = pred_label_idx.squeeze().item()\n",
    "class_names = ['Benign', 'Malignant']\n",
    "print(f\"Model Prediction: Class '{class_names[predicted_class_index]}' with confidence {prediction_score.item():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87289787-3b3a-4a7e-9edd-3f18c7554d48",
   "metadata": {},
   "source": [
    "### Grad-CAM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0b621-cf2c-4e8c-b572-24ac0a8d7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5a. Grad-CAM Implementation ---\n",
    "print(\"\\n--- Generating Grad-CAM ---\")\n",
    "\n",
    "# We target the initial convolutional layer that processes the image patches.\n",
    "# This layer is compatible with the standard Grad-CAM algorithm.\n",
    "grad_cam_layer = model.vit_model.vit.embeddings.patch_embeddings.projection\n",
    "\n",
    "def model_forward_wrapper(input_tensor):\n",
    "    \"\"\"Wrapper to return only the logits tensor from the model's output.\"\"\"\n",
    "    return model(input_tensor).logits\n",
    "\n",
    "layer_gc = LayerGradCam(model_forward_wrapper, grad_cam_layer)\n",
    "attribution_gc = layer_gc.attribute(input_tensor, target=predicted_class_index)\n",
    "\n",
    "# The output from the convolutional layer is already a 2D feature map,\n",
    "# so we can resize it directly without token reshaping.\n",
    "heatmap = F.interpolate(attribution_gc, size=input_image.size, mode='bilinear', align_corners=False)\n",
    "heatmap = heatmap.squeeze().cpu().detach().numpy()\n",
    "\n",
    "# Plotting and saving the Grad-CAM image\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.imshow(input_image)\n",
    "ax.imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "ax.axis('off')\n",
    "\n",
    "grad_cam_output_path = os.path.join(OUTPUT_DIR, 'user_sample1_xai_gradcam.png')\n",
    "plt.savefig(grad_cam_output_path, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n",
    "print(f\"Grad-CAM image saved to: {grad_cam_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ec665-c655-4970-9e87-ec589339e154",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6994d0-b7c0-4777-8531-5da1c6500268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5b. SHAP ---\n",
    "print(\"\\n--- Generating SHAP ---\")\n",
    "\n",
    "occlusion = Occlusion(model_forward_wrapper) # Use the wrapper here too for consistency\n",
    "attribution_shap = occlusion.attribute(input_tensor, strides=(3, 16, 16), target=predicted_class_index, sliding_window_shapes=(3, 32, 32), baselines=0)\n",
    "\n",
    "fig_shap, ax_shap = plt.subplots(figsize=(8, 8))\n",
    "ax_shap.axis('off')\n",
    "viz.visualize_image_attr(np.transpose(attribution_shap.squeeze().cpu().detach().numpy(), (1,2,0)), np.array(input_image.resize((224,224))), method=\"blended_heat_map\", sign=\"all\", show_colorbar=False, plt_fig_axis=(fig_shap, ax_shap))\n",
    "shap_output_path = os.path.join(OUTPUT_DIR, 'user_sample1_xai_shap.png')\n",
    "fig_shap.savefig(shap_output_path, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n",
    "print(f\"SHAP image saved to: {shap_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b00cb9-1160-4cbb-93ba-ec210d90a9d1",
   "metadata": {},
   "source": [
    "### Influence Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5385edee-8cea-45d6-90ed-4c26ef26c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5c. Influence Functions ---\n",
    "print(\"\\n--- Generating Influence Functions ---\")\n",
    "\n",
    "def get_gradient_and_prediction(model, data_tensor, target, loss_fn):\n",
    "    model.zero_grad()\n",
    "    logits = model(data_tensor).logits\n",
    "    _, pred_idx = torch.max(logits.data, 1)\n",
    "    loss = loss_fn(logits, target)\n",
    "    loss.backward()\n",
    "    grad = model.vit_model.classifier.weight.grad.detach().clone()\n",
    "    return grad.flatten(), pred_idx.item()\n",
    "\n",
    "def calculate_real_influence(model, train_loader, test_tensor, test_target, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_tensor = test_tensor.to(device)\n",
    "    test_target = torch.tensor([test_target]).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    print(\"Calculating gradient for the test image...\")\n",
    "    test_grad, _ = get_gradient_and_prediction(model, test_tensor, test_target, loss_fn)\n",
    "    results = []\n",
    "    print(\"Iterating through the training dataset...\")\n",
    "    for train_imgs, train_labels in tqdm(train_loader):\n",
    "        train_imgs, train_labels = train_imgs.to(device), train_labels.to(device)\n",
    "        for i in range(len(train_imgs)):\n",
    "            train_grad, train_pred = get_gradient_and_prediction(model, train_imgs[i].unsqueeze(0), train_labels[i].unsqueeze(0), loss_fn)\n",
    "            influence_score = torch.dot(test_grad, train_grad).item()\n",
    "            results.append({'score': influence_score, 'prediction': train_pred})\n",
    "    return results\n",
    "\n",
    "try:\n",
    "    print(f\"Loading processed training data from: {PROCESSED_TRAIN_DATA_PATH}\")\n",
    "    processed_data = torch.load(PROCESSED_TRAIN_DATA_PATH)\n",
    "    train_images, train_labels = processed_data['images'], processed_data['labels']\n",
    "    train_df = pd.read_csv(TRAIN_SPLIT_PATH)\n",
    "    train_filenames = train_df['DDI_file'].tolist()\n",
    "    train_dataset = TensorDataset(train_images, train_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    influence_results = calculate_real_influence(model, train_loader, input_tensor, predicted_class_index, device)\n",
    "    \n",
    "    report_data = [{'case_id': train_filenames[i].split('.')[0], 'influence_score': r['score'], 'ground_truth': train_labels[i].item(), 'prediction': r['prediction']} for i, r in enumerate(influence_results)]\n",
    "    report_df = pd.DataFrame(report_data)\n",
    "    report_df['abs_influence'] = report_df['influence_score'].abs()\n",
    "    report_df = report_df.sort_values(by='abs_influence', ascending=False).drop(columns='abs_influence')\n",
    "    final_report_df = report_df.head(100)\n",
    "\n",
    "    influence_output_path = os.path.join(OUTPUT_DIR, 'user_sample1_influence_function.csv')\n",
    "    final_report_df.to_csv(influence_output_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved {len(final_report_df)} real influence scores to: {influence_output_path}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: A required data file was not found. Please ensure notebooks 01 and 02 have been run. Details: {e}\")\n",
    "\n",
    "print(\"\\n\\n--- XAI Pipeline Execution Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
