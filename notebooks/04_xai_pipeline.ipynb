{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35ba11e-2ad2-4cbb-b595-34bc8b8bc8eb",
   "metadata": {},
   "source": [
    "# XAI Pipeline for Skin Cancer Classification\n",
    "This notebook loads the trained Xception model, applies there XAI methods (Grad-CAM, SHAP, Influence Functions), and exports the results in a format ready for the LLM integration pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a3447-fa5f-454c-b70c-32b19cf5c731",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369b6a0-dafd-47b4-9ef9-3b7e8820d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup and Initialization ---\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from captum.attr import LayerGradCam, Occlusion\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    print(f\"Project root added to path: {project_root}\")\n",
    "\n",
    "# Import the new helper functions\n",
    "from scd.utils.common import load_model, load_datasets, get_test_transforms\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe6cff5-5a8c-45ca-8623-91c502223415",
   "metadata": {},
   "source": [
    "## Define Project Structure and File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c80821-a160-46cb-a8dc-81c4e41c895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Define Project Structure and File Paths ---\n",
    "BASE_DIR = project_root\n",
    "MODEL_PATH = os.path.join(BASE_DIR, 'models', 'ResNet_skin_cancer_classification.pth')\n",
    "PROCESSED_DATA_DIR = os.path.join(BASE_DIR, 'data', 'processed')\n",
    "SAMPLE_IMAGE_PATH = os.path.join(BASE_DIR, 'user_inputs', 'user_sample1.jpg') # Assuming a sample image exists here\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'results', 'xai_output')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# New constant for image size\n",
    "IMAGE_RESIZE = (384, 384)\n",
    "\n",
    "print(f\"Project Base Directory: {BASE_DIR}\")\n",
    "print(f\"Using Model Path: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d96ef-09d5-4ad7-ae91-55cb5daab1c5",
   "metadata": {},
   "source": [
    "## Model Definition and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16bdb5b-2fc0-4956-9037-9f4f18bda9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Model Definition and Loading ---\n",
    "print(\"\\nLoading model...\")\n",
    "# Use the load_model helper function\n",
    "model = load_model(MODEL_PATH)\n",
    "model.eval()\n",
    "print(f\"Successfully loaded trained ResNet model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193fefa3-41ce-432a-8d4a-fd0e3f37ef5e",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e02957-3b87-461c-b2ab-d8b543042628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Data Loading and Preprocessing ---\n",
    "print(\"\\nLoading and preprocessing sample image...\")\n",
    "# Use the get_test_transforms helper function\n",
    "preprocess_transform = get_test_transforms(resize=IMAGE_RESIZE)\n",
    "\n",
    "try:\n",
    "    input_image = Image.open(SAMPLE_IMAGE_PATH).convert('RGB')\n",
    "    input_image = input_image.resize(IMAGE_RESIZE, resample=Image.BILINEAR)\n",
    "    image_np = np.array(input_image)\n",
    "    input_tensor = preprocess_transform(image=image_np)['image'].unsqueeze(0)\n",
    "    print(f\"Successfully loaded and preprocessed sample image: {SAMPLE_IMAGE_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Sample image not found at '{SAMPLE_IMAGE_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e79fee-f6b3-4a19-95e2-50623014ebc9",
   "metadata": {},
   "source": [
    "## XAI Pipeline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9ab77-681c-4509-8371-2d4500e66eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. XAI Pipeline Implementation ---\n",
    "print(\"\\n--- Running XAI Pipeline ---\")\n",
    "\n",
    "# Get model prediction\n",
    "outputs, attn_map = model(input_tensor)\n",
    "prediction_score, pred_label_idx = torch.topk(outputs, 1)\n",
    "predicted_class_index = pred_label_idx.squeeze().item()\n",
    "class_names = ['Benign', 'Malignant']\n",
    "print(f\"Model Prediction: Class '{class_names[predicted_class_index]}' with confidence {prediction_score.item():.2%}\")\n",
    "\n",
    "# Create and save the model output DataFrame\n",
    "probabilities = torch.softmax(outputs, dim=1) # apply softmax\n",
    "benign_prob = probabilities[0][0].item()\n",
    "malignant_prob = probabilities[0][1].item()\n",
    "\n",
    "# Create a list of dictionaries for the two rows\n",
    "output_data = [\n",
    "    {'class': 'Benign', 'confidence': benign_prob},\n",
    "    {'class': 'Malignant', 'confidence': malignant_prob}\n",
    "]\n",
    "output_df = pd.DataFrame(output_data)\n",
    "\n",
    "# Save the file\n",
    "model_output_path = os.path.join(OUTPUT_DIR, 'model_output.csv')\n",
    "output_df.to_csv(model_output_path, index=False)\n",
    "print(f\"Model output saved to: {model_output_path}\")\n",
    "\n",
    "# Wrapper function for Captum\n",
    "def model_forward_wrapper(input_tensor):\n",
    "    \"\"\"Wrapper to return only the logits tensor from the model's output.\"\"\"\n",
    "    return model(input_tensor)[0] # Return only the first element (the predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de256a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5a. Built-in Attention Map Visualization ---\n",
    "print(\"\\n--- Visualizing Model's Built-in Attention Map ---\")\n",
    "\n",
    "# Upsample attention map to input image size\n",
    "upsampled_attn = F.interpolate(attn_map, size=IMAGE_RESIZE, mode='bilinear', align_corners=False)\n",
    "\n",
    "# Convert to numpy and normalize to [0, 1] range\n",
    "attn_np = upsampled_attn.squeeze().cpu().detach().numpy()\n",
    "attn_np = (attn_np - attn_np.min()) / (attn_np.max() - attn_np.min())\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot original image\n",
    "axes[0].imshow(input_image)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Plot attention heatmap\n",
    "im = axes[1].imshow(attn_np, cmap='jet')\n",
    "axes[1].set_title(\"Attention Map\")\n",
    "axes[1].axis(\"off\")\n",
    "cbar = fig.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Attention Intensity', rotation=270, labelpad=15)\n",
    "\n",
    "# Create overlay of attention on original image\n",
    "# Convert input image to numpy for overlay\n",
    "input_np = np.array(input_image)\n",
    "# Create a colored heatmap using matplotlib's colormap\n",
    "cmap = plt.cm.jet\n",
    "heatmap_colored = cmap(attn_np)[:, :, :3]  # Get RGB from colormap (drop alpha)\n",
    "# Create overlay (blend original image with heatmap)\n",
    "overlay = 0.7 * input_np / 255.0 + 0.3 * heatmap_colored\n",
    "\n",
    "# Plot overlay\n",
    "axes[2].imshow(overlay)\n",
    "axes[2].set_title(\"Overlay (30% attention, 70% image)\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the visualization\n",
    "grad_cam_output_path = os.path.join(OUTPUT_DIR, 'sample3_attn.png')\n",
    "plt.savefig(grad_cam_output_path, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87289787-3b3a-4a7e-9edd-3f18c7554d48",
   "metadata": {},
   "source": [
    "### Grad-CAM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0b621-cf2c-4e8c-b572-24ac0a8d7b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5a. Grad-CAM Implementation ---\n",
    "print(\"\\n--- Generating Grad-CAM ---\")\n",
    "# Target layer for ResNet model from scd/model.py\n",
    "grad_cam_layer = model.features[-1] \n",
    "layer_gc = LayerGradCam(model_forward_wrapper, grad_cam_layer) # Use the wrapper\n",
    "attribution_gc = layer_gc.attribute(input_tensor, target=predicted_class_index)\n",
    "\n",
    "# Upsample the heatmap to the original image size\n",
    "heatmap = F.interpolate(attribution_gc, size=input_image.size, mode='bilinear', align_corners=False)\n",
    "heatmap = heatmap.squeeze().cpu().detach().numpy()\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.set_title('Grad-CAM Visualisation')\n",
    "ax.imshow(input_image)\n",
    "# Capture the image object returned by imshow to use in the color bar\n",
    "im = ax.imshow(heatmap, cmap='jet', alpha=0.5) \n",
    "ax.axis('off')\n",
    "\n",
    "# Add the color bar to the figure\n",
    "cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Importance for Predicted Class', rotation=270, labelpad=20)\n",
    "\n",
    "# Save and show the plot\n",
    "grad_cam_output_path = os.path.join(OUTPUT_DIR, 'user_sample1_xai_gradcam.png')\n",
    "plt.savefig(grad_cam_output_path, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n",
    "print(f\"Grad-CAM image saved to: {grad_cam_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ec665-c655-4970-9e87-ec589339e154",
   "metadata": {},
   "source": [
    "### SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6994d0-b7c0-4777-8531-5da1c6500268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5b. SHAP (using Occlusion) ---\n",
    "print(\"\\n--- Generating SHAP ---\")\n",
    "occlusion = Occlusion(model_forward_wrapper) # Use the wrapper here\n",
    "# Adjust sliding window shapes for the new 384x384 size\n",
    "attribution_shap = occlusion.attribute(input_tensor, \n",
    "                                       strides=(3, 32, 32), \n",
    "                                       target=predicted_class_index, \n",
    "                                       sliding_window_shapes=(3, 48, 48),\n",
    "                                       baselines=0)\n",
    "\n",
    "# The visualize_image_attr function creates the plot and color bar\n",
    "fig, _ = viz.visualize_image_attr(\n",
    "    attr=np.transpose(attribution_shap.squeeze().cpu().detach().numpy(), (1,2,0)), \n",
    "    original_image=np.array(input_image.resize(IMAGE_RESIZE)), \n",
    "    method=\"blended_heat_map\", \n",
    "    sign=\"all\", \n",
    "    show_colorbar=True,\n",
    "    title=\"SHAP Explanation\"\n",
    ")\n",
    "\n",
    "# Save and show the plot\n",
    "shap_output_path = os.path.join(OUTPUT_DIR, 'user_sample1_xai_shap.png')\n",
    "fig.savefig(shap_output_path, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n",
    "print(f\"SHAP image saved to: {shap_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b00cb9-1160-4cbb-93ba-ec210d90a9d1",
   "metadata": {},
   "source": [
    "### Influence Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5385edee-8cea-45d6-90ed-4c26ef26c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5c. Influence Functions ---\n",
    "print(\"\\n--- Generating Influence Functions ---\")\n",
    "def get_gradient_and_prediction(model, data_tensor, target, loss_fn):\n",
    "    model.zero_grad()\n",
    "    # Use the wrapper to get logits\n",
    "    logits = model_forward_wrapper(data_tensor)\n",
    "    _, pred_idx = torch.max(logits.data, 1)\n",
    "    loss = loss_fn(logits, target)\n",
    "    loss.backward()\n",
    "    # Get gradient from the final classifier layer of the ResNet model\n",
    "    grad = model.classifier[-1].weight.grad.detach().clone()\n",
    "    return grad.flatten(), pred_idx.item()\n",
    "\n",
    "def calculate_real_influence(model, train_loader, test_tensor, test_target, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_tensor = test_tensor.to(device)\n",
    "    test_target = torch.tensor([test_target]).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"Calculating gradient for the test image...\")\n",
    "    test_grad, _ = get_gradient_and_prediction(model, test_tensor, test_target, loss_fn)\n",
    "    \n",
    "    results = []\n",
    "    print(\"Iterating through the training dataset to calculate influence scores...\")\n",
    "    for (train_imgs, train_labels, filenames) in tqdm(train_loader):\n",
    "        train_imgs, train_labels = train_imgs.to(device), train_labels.to(device)\n",
    "        for i in range(len(train_imgs)):\n",
    "            train_grad, train_pred = get_gradient_and_prediction(model, train_imgs[i].unsqueeze(0), train_labels[i].unsqueeze(0), loss_fn)\n",
    "            influence_score = torch.dot(test_grad, train_grad).item()\n",
    "            results.append({'score': influence_score, 'prediction': train_pred, 'filename': filenames[i]})\n",
    "    return results\n",
    "\n",
    "try:\n",
    "    print(f\"Loading processed training data from: {PROCESSED_DATA_DIR}\")\n",
    "    # Load data using the new helper function\n",
    "    train_dataset, train_filenames = load_datasets(PROCESSED_DATA_DIR, only_train_dataset_with_filenames=True)\n",
    "    \n",
    "    # Create a new TensorDataset that includes filenames for the loader\n",
    "    class DatasetWithFilenames(TensorDataset):\n",
    "        def __init__(self, tensors, filenames):\n",
    "            super(DatasetWithFilenames, self).__init__(*tensors)\n",
    "            self.filenames = filenames\n",
    "        def __getitem__(self, index):\n",
    "            return super(DatasetWithFilenames, self).__getitem__(index) + (self.filenames[index],)\n",
    "\n",
    "    train_images_tensors, train_labels_tensors = train_dataset.tensors\n",
    "    train_dataset_with_files = DatasetWithFilenames((train_images_tensors, train_labels_tensors), train_filenames)\n",
    "    train_loader = DataLoader(train_dataset_with_files, batch_size=32)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    influence_results = calculate_real_influence(model, train_loader, input_tensor, predicted_class_index, device)\n",
    "    \n",
    "    # The ground truth is now part of the dataset, not a separate CSV\n",
    "    gt_map = {fname: label.item() for fname, label in zip(train_filenames, train_labels_tensors)}\n",
    "\n",
    "    report_data = [{'case_id': r['filename'].split('.')[0], \n",
    "                    'influence_score': r['score'], \n",
    "                    'ground_truth': 'Malignant' if gt_map.get(r['filename'], -1) == 1 else 'Benign', # Look up ground truth\n",
    "                    'prediction': 'Malignant' if r['prediction'] == 1 else 'Benign'} for r in influence_results]\n",
    "    \n",
    "    report_df = pd.DataFrame(report_data)\n",
    "    report_df['abs_influence'] = report_df['influence_score'].abs()\n",
    "    report_df = report_df.sort_values(by='abs_influence', ascending=False).drop(columns='abs_influence')\n",
    "    final_report_df = report_df.head(100)\n",
    "\n",
    "    influence_output_path = os.path.join(OUTPUT_DIR, 'user_sample1_influence_function.csv')\n",
    "    final_report_df.to_csv(influence_output_path, index=False)\n",
    "    print(f\"\\nSuccessfully saved {len(final_report_df)} influence scores to: {influence_output_path}\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: A required data file was not found. Please ensure notebooks 01 and 02 have been run. Details: {e}\")\n",
    "\n",
    "print(\"\\n\\n--- XAI Pipeline Execution Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
