{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9be021",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2d6a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dango\\OneDrive - UWE Bristol\\projects\\thesis-project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# System libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Local imports\n",
    "from scd.utils.common import get_test_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e3c323",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8774769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "# Define paths\n",
    "root_path = Path.cwd().parent\n",
    "data_path = root_path / 'data'\n",
    "image_path = data_path / 'raw_dataset' / 'images'\n",
    "augmented_image_path = data_path / 'augmented_images'\n",
    "metadata_path = data_path / 'metadata_updated.csv'\n",
    "\n",
    "model_type = 'ViT'\n",
    "resize = (224, 224) if model_type == 'ViT' else (299, 299)  # (299, 299) for Xception, (224, 224) for Vision Transformer\n",
    "# resize = (384, 384)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79293f6",
   "metadata": {},
   "source": [
    "## Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54cfae54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DDI_ID</th>\n",
       "      <th>image_id</th>\n",
       "      <th>skin_tone</th>\n",
       "      <th>malignant</th>\n",
       "      <th>disease</th>\n",
       "      <th>strata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>000001.png</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>melanoma-in-situ</td>\n",
       "      <td>56_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>000002.png</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>melanoma-in-situ</td>\n",
       "      <td>56_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>000003.png</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>mycosis-fungoides</td>\n",
       "      <td>56_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>000004.png</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>squamous-cell-carcinoma-in-situ</td>\n",
       "      <td>56_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>000005.png</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>basal-cell-carcinoma</td>\n",
       "      <td>12_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  DDI_ID    image_id  skin_tone  malignant  \\\n",
       "0           0       1  000001.png         56          1   \n",
       "1           1       2  000002.png         56          1   \n",
       "2           2       3  000003.png         56          1   \n",
       "3           3       4  000004.png         56          1   \n",
       "4           4       5  000005.png         12          1   \n",
       "\n",
       "                           disease strata  \n",
       "0                 melanoma-in-situ   56_1  \n",
       "1                 melanoma-in-situ   56_1  \n",
       "2                mycosis-fungoides   56_1  \n",
       "3  squamous-cell-carcinoma-in-situ   56_1  \n",
       "4             basal-cell-carcinoma   12_1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load metadata\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd280c24",
   "metadata": {},
   "source": [
    "## Split Training, Validation and Testing Set\n",
    "\n",
    "We split the dataset into training (60%), validation (20%) and testing (20%) sets using stratified sampling to ensure balanced distribution of malignant and benign cases across all splits. This approach maintains the same proportion of classes in each subset, which is important for model training and evaluation, especially with imbalanced datasets.\n",
    "\n",
    "We first split the data into train (60%) and a temporary set (40%), then further divide the temporary set into validation and test sets of equal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "771cee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_df, val_df = train_test_split(\n",
    "  metadata, \n",
    "  test_size=0.4, \n",
    "  stratify=metadata['malignant'], \n",
    "  random_state=random_state\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "  val_df,\n",
    "  test_size=0.5,\n",
    "  stratify=val_df['malignant'],\n",
    "  random_state=random_state\n",
    ")\n",
    "\n",
    "# Define the directory for processed data\n",
    "processed_dir = root_path / 'data' / 'processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9879ec",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "We create a custom PyTorch dataset class (`SkinDataset`) to efficiently load and preprocess skin lesion images for our deep learning model. The dataset class handles:\n",
    "\n",
    "1. Loading images from file paths stored in our metadata DataFrame\n",
    "2. Applying provided transformations to the images\n",
    "3. Pairing each image with its corresponding label (malignant or benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97e186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading skin images and their labels.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe: pd.DataFrame, data_dir: str, transform: callable):\n",
    "        \"\"\"\n",
    "        Initializes the SkinDataset with a DataFrame, data directory, and transformations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataframe : pandas.DataFrame\n",
    "            DataFrame containing image file paths and labels.\n",
    "        data_dir : str\n",
    "            Directory where raw and augmented images are stored.\n",
    "        transform : callable\n",
    "            Transformations to apply to the images.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.raw_dataset_path = data_dir / 'raw_dataset' / 'images'\n",
    "        self.aug_dataset_path = data_dir / 'augmented_images'\n",
    "        self.ham_dataset_path = data_dir / 'raw_dataset' / 'HAM10000' / 'images'\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\" Returns the number of samples in the dataset. \"\"\"\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Retrieves an image and its label by index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the sample to retrieve.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple containing the transformed image and its label.\n",
    "        \"\"\"\n",
    "        # Create image path with image directory and filename\n",
    "        filename = str(self.dataframe.loc[idx, 'image_id'])\n",
    "        if 'aug' in filename:\n",
    "            img_path = os.path.join(self.aug_dataset_path, filename)\n",
    "        elif 'ISIC' in filename:\n",
    "            img_path = os.path.join(self.ham_dataset_path, f'{filename}.jpg')\n",
    "        else:\n",
    "            img_path = os.path.join(self.raw_dataset_path, filename)\n",
    "        \n",
    "        # Retrieve the label for the image\n",
    "        label = self.dataframe.loc[idx, 'malignant']\n",
    "\n",
    "        # Load the image, convert to RGB, and apply transformations\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image_np = np.array(image)\n",
    "        image = self.transform(image=image_np)['image']\n",
    "\n",
    "        # Return the transformed image and its label and filename\n",
    "        return image, label, filename\n",
    "    \n",
    "    def get_labels(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns the labels of the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Array of labels.\n",
    "        \"\"\"\n",
    "        return self.dataframe['malignant'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b1197",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "The `transformation()` function creates two separate transformation pipelines:\n",
    "\n",
    "1. **Training Transformations:** Apply various random modifications to training images to help the model learn more robust features:\n",
    "  - Resize images to 299Ã—299 pixels (Xception's required input size)\n",
    "  - Random horizontal and vertical flips to simulate different orientations\n",
    "  - Random affine transformations (rotation, translation, scaling) to provide positional variance\n",
    "  - Color jitter to simulate lighting variations\n",
    "  - Normalisation with ImageNet mean and standard deviation values\n",
    "  - Random erasing to help the model learn to identify lesions even with partial occlusions\n",
    "  - Gaussian blur to simulate focus variations in dermatoscopic images\n",
    "\n",
    "2. **Test/Validation Transformations:** Apply only essential preprocessing:\n",
    "  - Resize images to the required dimensions\n",
    "  - Normalisation to match training data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f8ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformations(resize: tuple, for_augmentation=False) -> A.Compose:\n",
    "    \"\"\"\n",
    "    Converts torchvision-style transforms to Albumentations-based transforms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    resize : tuple\n",
    "        Target size in (height, width) format.\n",
    "    for_augmentation : bool\n",
    "        If True, returns only the training transformation for augmentation purposes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    albumentations.Compose\n",
    "        The augmentation or training transformations.\n",
    "    \"\"\"\n",
    "    if for_augmentation:\n",
    "        # If for augmentation, only return the training transform\n",
    "        return A.Compose([\n",
    "            A.Resize(height=resize[0], width=resize[1]),\n",
    "            A.HorizontalFlip(p=0.2),\n",
    "            A.VerticalFlip(p=0.2),\n",
    "            A.Affine(rotate=(-20, 20), translate_percent=(0.1, 0.1), scale=(0.9, 1.1), p=0.8),\n",
    "            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.0, p=0.5),\n",
    "            A.GaussianBlur(blur_limit=(5, 5), sigma_limit=(0.1, 2.0), p=0.3),\n",
    "            A.CoarseDropout(max_holes=8, max_height=int(resize[0]*0.1), max_width=int(resize[1]*0.1), p=0.2),\n",
    "        ]), None\n",
    "\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(height=resize[0], width=resize[1]),\n",
    "        A.HorizontalFlip(p=0.2),\n",
    "        A.VerticalFlip(p=0.2),\n",
    "        A.Affine(rotate=(-20, 20), translate_percent=(0.1, 0.1), scale=(0.9, 1.1), p=0.8),\n",
    "        A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.0, p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(5, 5), sigma_limit=(0.1, 2.0), p=0.3),\n",
    "        A.CoarseDropout(max_holes=8, max_height=int(resize[0]*0.1), max_width=int(resize[1]*0.1), p=0.2),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) if not for_augmentation else lambda x: x,\n",
    "        ToTensorV2() if not for_augmentation else lambda x: x,\n",
    "    ])\n",
    "\n",
    "    return train_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995b5c6",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data augmentation is a crucial technique for improving model generalisation and performance, especially when working with limited datasets. Our augmentation function applies various transformations to the training data to artificially increase the diversity of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b4f9f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(df, img_path, output_path, transform, num_augmented=3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies Albumentations transform multiple times to each image, \n",
    "    saves the augmented versions and combines the new metadata with the original.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with 'filename' and 'label' columns.\n",
    "    img_path : str\n",
    "        Path to data folder.\n",
    "    output_path : str\n",
    "        Path to save augmented images.\n",
    "    transform : albumentations.Compose\n",
    "        The train_transform pipeline.\n",
    "    num_augmented : int\n",
    "        Number of augmented versions per image.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        New combined DataFrame with filenames and labels of augmented images.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    new_records = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        filename = row['image_id']\n",
    "        label = row['malignant']\n",
    "\n",
    "        image = Image.open(f\"{img_path}/{filename}\").convert(\"RGB\")\n",
    "        image_np = np.array(image)\n",
    "\n",
    "        for i in range(num_augmented):\n",
    "            augmented = transform(image=image_np)['image']\n",
    "\n",
    "            new_filename = f\"{os.path.splitext(filename)[0]}_aug{i}.png\"\n",
    "            save_path = os.path.join(output_path, new_filename)\n",
    "            Image.fromarray(augmented).save(save_path)\n",
    "\n",
    "            new_records.append({'image_id': new_filename, 'malignant': label})\n",
    "    \n",
    "    augmented_df = pd.DataFrame(new_records)\n",
    "\n",
    "    print(\"Sample of augmented data:\")\n",
    "    print(augmented_df.head())\n",
    "    print('\\n\\n')\n",
    "\n",
    "    # Return combined original and augmented data\n",
    "    return pd.concat([train_df, augmented_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c188b5b0",
   "metadata": {},
   "source": [
    "## Combine with HAM Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e42140da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ham_dataset(df_path: Path, image_path: Path, transform: A.Compose) -> SkinDataset:\n",
    "    \"\"\"\n",
    "    Creates a SkinDataset instance with the HAM dataset.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : Path\n",
    "        Path to the data directory containing the HAM dataset.\n",
    "    image_path : Path\n",
    "        Path to the directory containing the images of the HAM dataset.\n",
    "    transform : A.Compose\n",
    "        Transformations to apply to the images.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    SkinDataset\n",
    "        An instance of the SkinDataset class with HAM dataset.\n",
    "    \"\"\"\n",
    "    # Load the HAM dataset metadata\n",
    "    ham_df = pd.read_csv(df_path)\n",
    "\n",
    "    # Create the SkinDataset instance of the HAM dataset\n",
    "    ham_dataset = SkinDataset(\n",
    "        dataframe=ham_df,\n",
    "        data_dir=image_path,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    return ham_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285036d",
   "metadata": {},
   "source": [
    "## Create Datasets\n",
    "\n",
    "The `create_datasets()` function creates Skin Dataset objects for training, validation, and testing applying the specified data augmentation transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d66ee0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dataframes: tuple, augmented_image_path: Path, resize: tuple) -> tuple:\n",
    "    \"\"\"\n",
    "    Create datasets for training, validation, and testing. Uses augmentation transformations as defined in this project.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframes : tuple\n",
    "        A tuple containing three DataFrames: (train_df, val_df, test_df).\n",
    "    augmented_image_path : Path\n",
    "        Path to the directory where augmented images are stored.\n",
    "    resize : tuple\n",
    "        Tuple specifying the size to which images should be resized (height, width).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of SkinDataset\n",
    "        A tuple containing the training, validation, and test datasets.\n",
    "    \"\"\"\n",
    "    train_df, val_df, test_df = dataframes\n",
    "\n",
    "    # Initialise transformations\n",
    "    train_transform = transformations(resize=resize)\n",
    "    test_transform = get_test_transforms(resize=resize)\n",
    "    # augment_transform, _ = transformations(resize=resize, for_augmentation=True)\n",
    "    # combined_train_df = augmentation(train_df, image_path, augmented_image_path, augment_transform)\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = SkinDataset(train_df, data_path, transform=train_transform)\n",
    "    val_dataset = SkinDataset(val_df, data_path, transform=test_transform)\n",
    "    test_dataset = SkinDataset(test_df, data_path, transform=test_transform)\n",
    "\n",
    "    # Return the datasets\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44303c27",
   "metadata": {},
   "source": [
    "## Store Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be4dd256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# Store as PyTorch tensors\n",
    "def store_dataset(dataset: Dataset, name: str, output_dir: Path, include_ham: bool = False) -> None:\n",
    "  \"\"\" \n",
    "  Store a PyTorch dataset as tensors in a specified directory.\n",
    "  Parameters\n",
    "  ----------\n",
    "  dataset : Dataset\n",
    "      The dataset to be stored.\n",
    "  name : str\n",
    "      Name of the dataset to be saved.\n",
    "  output_dir : Path\n",
    "      Directory where the dataset will be saved.\n",
    "  include_ham : bool\n",
    "      If True, includes the HAM dataset in the saved tensors.\n",
    "  \"\"\"\n",
    "  if include_ham:\n",
    "    ham_df = pd.read_csv(data_path / 'ham_metadata_updated.csv')\n",
    "\n",
    "    ham_dataset = SkinDataset(\n",
    "      dataframe=ham_df,\n",
    "      data_dir=data_path,\n",
    "      transform=transformations(resize=resize, for_augmentation=False)\n",
    "    )\n",
    "    dataset = ConcatDataset([dataset, ham_dataset])\n",
    "\n",
    "  images = []\n",
    "  labels = []\n",
    "  filenames = []\n",
    "  for i in range(len(dataset)):\n",
    "    image, label, filename = dataset[i]\n",
    "    images.append(image)\n",
    "    labels.append(label)\n",
    "    filenames.append(filename)\n",
    "  \n",
    "  # Convert to tensors\n",
    "  images = torch.stack(images)\n",
    "  labels = torch.tensor(labels)\n",
    "  filenames = np.array(filenames)\n",
    "  \n",
    "  # Save tensors\n",
    "  torch.save({\n",
    "    'images': images,\n",
    "    'labels': labels,\n",
    "    'filenames': filenames\n",
    "  }, output_dir / f'{name}_dataset.pt')\n",
    "  \n",
    "  print(f\"Saved {name} dataset with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec784b",
   "metadata": {},
   "source": [
    "## Execute Create and Store Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b63668af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dango\\AppData\\Local\\Temp\\ipykernel_24628\\1636183070.py:36: UserWarning: Argument(s) 'max_holes, max_height, max_width' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=8, max_height=int(resize[0]*0.1), max_width=int(resize[1]*0.1), p=0.2),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train dataset with 393 samples\n",
      "Saved val dataset with 131 samples\n",
      "Saved test dataset with 132 samples\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset, val_dataset, test_dataset = create_datasets((train_df, val_df, test_df), augmented_image_path, resize=resize)\n",
    "\n",
    "# Create directory for saving loaders if it doesn't exist\n",
    "save_dir = root_path / 'data' / 'processed' / model_type\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save all datasets\n",
    "store_dataset(train_dataset, 'train', save_dir, include_ham=False)\n",
    "store_dataset(val_dataset, 'val', save_dir)\n",
    "store_dataset(test_dataset, 'test', save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
