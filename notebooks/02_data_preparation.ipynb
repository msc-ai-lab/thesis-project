{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9be021",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2d6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e3c323",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8774769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "# Define paths\n",
    "root_path = Path.cwd().parent\n",
    "metadata_path = root_path / 'data' / 'metadata_updated.csv'\n",
    "image_path = root_path / 'data' / 'raw_dataset' / 'images'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79293f6",
   "metadata": {},
   "source": [
    "## Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54cfae54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>DDI_ID</th>\n",
       "      <th>DDI_file</th>\n",
       "      <th>skin_tone</th>\n",
       "      <th>malignant</th>\n",
       "      <th>disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>000001.png</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>melanoma-in-situ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>000002.png</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>melanoma-in-situ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>000003.png</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>mycosis-fungoides</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>000004.png</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>squamous-cell-carcinoma-in-situ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>000005.png</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>basal-cell-carcinoma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  DDI_ID    DDI_file  skin_tone  malignant  \\\n",
       "0           0       1  000001.png         56          1   \n",
       "1           1       2  000002.png         56          1   \n",
       "2           2       3  000003.png         56          1   \n",
       "3           3       4  000004.png         56          1   \n",
       "4           4       5  000005.png         12          1   \n",
       "\n",
       "                           disease  \n",
       "0                 melanoma-in-situ  \n",
       "1                 melanoma-in-situ  \n",
       "2                mycosis-fungoides  \n",
       "3  squamous-cell-carcinoma-in-situ  \n",
       "4             basal-cell-carcinoma  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load metadata\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd280c24",
   "metadata": {},
   "source": [
    "## Split Training, Validation and Testing Set\n",
    "\n",
    "We split the dataset into training (60%), validation (20%) and testing (20%) sets using stratified sampling to ensure balanced distribution of malignant and benign cases across all splits. This approach maintains the same proportion of classes in each subset, which is important for model training and evaluation, especially with imbalanced datasets.\n",
    "\n",
    "We first split the data into train (60%) and a temporary set (40%), then further divide the temporary set into validation and test sets of equal size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "771cee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_df, val_df = train_test_split(\n",
    "  metadata, \n",
    "  test_size=0.4, \n",
    "  stratify=metadata['malignant'], \n",
    "  random_state=random_state\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "  val_df,\n",
    "  test_size=0.5,\n",
    "  stratify=val_df['malignant'],\n",
    "  random_state=random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9879ec",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "We create a custom PyTorch dataset class (`SkinDataset`) to efficiently load and preprocess skin lesion images for our deep learning model. The dataset class handles:\n",
    "\n",
    "1. Loading images from file paths stored in our metadata DataFrame\n",
    "2. Applying provided transformations to the images\n",
    "3. Pairing each image with its corresponding label (malignant or benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97e186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading skin images and their labels.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe: pd.DataFrame, img_dir: str, transform: callable):\n",
    "        \"\"\"\n",
    "        Initializes the SkinDataset with a DataFrame, image directory, and transformations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataframe : pandas.DataFrame\n",
    "            DataFrame containing image file paths and labels.\n",
    "        img_dir : str\n",
    "            Directory where images are stored.\n",
    "        transform : callable\n",
    "            Transformations to apply to the images.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\" Returns the number of samples in the dataset. \"\"\"\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Retrieves an image and its label by index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of the sample to retrieve.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple containing the transformed image and its label.\n",
    "        \"\"\"\n",
    "        # Create image path with image directory and filename\n",
    "        img_path = Path(self.img_dir) / self.dataframe.loc[idx, 'DDI_file']\n",
    "\n",
    "        # Retrieve the label for the image\n",
    "        label = self.dataframe.loc[idx, 'malignant']\n",
    "\n",
    "        # Load the image, convert to RGB, and apply transformations\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # Return the transformed image and its label\n",
    "        return image, label\n",
    "    \n",
    "    def get_labels(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns the labels of the dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Array of labels.\n",
    "        \"\"\"\n",
    "        return self.dataframe['malignant'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b1197",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data augmentation is a crucial technique for improving model generalisation and performance, especially when working with limited datasets. Our augmentation function applies various transformations to the training data to artificially increase the diversity of the training set.\n",
    "\n",
    "The `augmentation()` function creates two separate transformation pipelines:\n",
    "\n",
    "1. **Training Transformations:** Apply various random modifications to training images to help the model learn more robust features:\n",
    "  - Resize images to 299Ã—299 pixels (Xception's required input size)\n",
    "  - Random horizontal and vertical flips to simulate different orientations\n",
    "  - Random affine transformations (rotation, translation, scaling) to provide positional variance\n",
    "  - Color jitter to simulate lighting variations\n",
    "  - Normalisation with ImageNet mean and standard deviation values\n",
    "  - Random erasing to help the model learn to identify lesions even with partial occlusions\n",
    "  - Gaussian blur to simulate focus variations in dermatoscopic images\n",
    "\n",
    "2. **Test/Validation Transformations:** Apply only essential preprocessing:\n",
    "  - Resize images to the required dimensions\n",
    "  - Normalisation to match training data distribution\n",
    "\n",
    "Each transformation can be toggled on or off using function parameters, allowing for controlled experimentation with different augmentation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f8ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(resize: tuple) -> tuple:\n",
    "    \"\"\"\n",
    "    Defines image transformations for training and testing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    resize : tuple\n",
    "        Tuple specifying the size to which images should be resized (height, width).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of torchvision.transforms.Compose\n",
    "        A tuple containing (train_transform, test_transform) for training and testing datasets.\n",
    "    \"\"\"\n",
    "    # Define image transformations for training\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(resize),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomVerticalFlip(p=0.2),\n",
    "        transforms.RandomAffine(degrees=20, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.1)),\n",
    "        transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "    ])\n",
    "\n",
    "    # Define image transformations for testing\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(resize),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Return tuple of train_transform and test_transform\n",
    "    return train_transform, test_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285036d",
   "metadata": {},
   "source": [
    "## Create Data Loaders\n",
    "\n",
    "The `create_data_loaders()` function creates PyTorch DataLoader objects for training, validation, and testing datasets applying the specified data augmentation transformations. These DataLoaders efficiently batch and shuffle data during model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d66ee0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(resize: tuple, batch_size: int = 32, oversample: bool = False) -> tuple:\n",
    "    \"\"\"\n",
    "    Create data loaders for training, validation, and testing datasets. Uses augmentation transformations as provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    resize : tuple\n",
    "        Tuple specifying the size to which images should be resized (height, width).\n",
    "    batch_size : int, optional\n",
    "        Batch size for the data loaders. Defaults to 32.\n",
    "    oversample : bool, optional\n",
    "        Whether to oversample the training dataset. Defaults to False.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple of torch.utils.data.DataLoader\n",
    "        A tuple containing the training, validation, and testing data loaders.\n",
    "    \"\"\"\n",
    "    # Initialise transformations\n",
    "    train_transform, test_transform = augmentation(resize=resize)\n",
    "\n",
    "    from torchsampler import ImbalancedDatasetSampler\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = SkinDataset(train_df, image_path, transform=train_transform)\n",
    "    val_dataset = SkinDataset(val_df, image_path, transform=test_transform)\n",
    "    test_dataset = SkinDataset(test_df, image_path, transform=test_transform)\n",
    "\n",
    "    # Create data loaders\n",
    "    if oversample:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=ImbalancedDatasetSampler(train_dataset))\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Return the data loaders\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be4dd256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train dataset with 393 samples\n",
      "Saved val dataset with 131 samples\n",
      "Saved test dataset with 132 samples\n"
     ]
    }
   ],
   "source": [
    "model_type = 'ViT'\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "  # resize=(299, 299), # Xception\n",
    "  resize=(224, 224), # Vision Transformer\n",
    "  batch_size=32\n",
    ")\n",
    "\n",
    "# Create directory for saving loaders if it doesn't exist\n",
    "save_dir = root_path / 'data' / 'processed' / model_type\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Extract datasets from loaders\n",
    "train_dataset = train_loader.dataset\n",
    "val_dataset = val_loader.dataset\n",
    "test_dataset = test_loader.dataset\n",
    "\n",
    "# Extract data and save as PyTorch tensors\n",
    "# (Note: This extracts all data into memory which may be slow for large datasets)\n",
    "def extract_and_save_dataset(dataset, name):\n",
    "  images = []\n",
    "  labels = []\n",
    "  for i in range(len(dataset)):\n",
    "    image, label = dataset[i]\n",
    "    images.append(image)\n",
    "    labels.append(label)\n",
    "  \n",
    "  # Convert to tensors\n",
    "  images = torch.stack(images)\n",
    "  labels = torch.tensor(labels)\n",
    "  \n",
    "  # Save tensors\n",
    "  torch.save({\n",
    "    'images': images,\n",
    "    'labels': labels\n",
    "  }, save_dir / f'{name}_dataset.pt')\n",
    "  \n",
    "  print(f\"Saved {name} dataset with {len(dataset)} samples\")\n",
    "\n",
    "# Save all datasets\n",
    "extract_and_save_dataset(train_dataset, 'train')\n",
    "extract_and_save_dataset(val_dataset, 'val')\n",
    "extract_and_save_dataset(test_dataset, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
